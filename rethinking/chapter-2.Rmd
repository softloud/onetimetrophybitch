---
title: "chapter 2"
description: |
  exercises from chapter 2 statistical rethinking
date: "`r Sys.Date()`"
output: 
    distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## easy

### 2E1

> Which of the expressions below correspond to the statement: the probability of rain on Monday?

1. P(rain)
2. P(rain|Monday)
3. P(Monday|rain)
4. P(rain, Monday) / P(Monday)

Adapting the equation on p. 37 to a more generalised statement, for a parameter of interest $\theta$ and observations $y$ dependent on that parameter, we have

$$
P(\theta | y) = \frac{P(y|\theta)P(\theta)}{P(y)} = P(\theta, y)/P(y)
$$

Then P(rain|Monday) = P(rain, Monday) / P(Monday).

So, I conclude that 1. and 4. correspond to the statement.

### 2E2

The following statement corresponds to the expression P(Monday|rain):

3. The probability that it is Monday, given that it is raining. 

### 2E3

> The probability that it is Monday given that it is raining.

Can be expressed


P(Monday|rain) = P(rain|Monday)P(Monday)/P(rain)

So, the following statements correspond.

1. P(Monday|rain)
4. P(rain|Monday)P(Monday)/P(rain)

## medium

### 2M1

> Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for **p**.

1. WWW
2. WWWL
3. LWWLWWW

We assume 
$$
\begin{array}
\\W & \sim & \text{binomial}(N, p)\\
p & \sim & \text{uniform}(0, 1)
\end{array}
$$
where $W$ is the number of water observations and $N = L + W$ is the total number of water and land observations. We only know $p$ is a proportion, so we know that it falls in $[0,1]$.

We are interested in finding
$$
P(p|W,L) = P(W,L|p)P(p)/P(W,L)
$$
where $P(p|W,L)$ is the posterior, $P(W,L|p)$ is the probability of the data, $P(W,L)$ is the prior.

> And this is Bayes' theorem. It says the the probability of any particular value of $p$, considering the data is equal to the product of the relative plausibility of the data, conditional on $p$, and the prior plausibility of $p$, divided by the average probability of the data. 

$$
\text{Posterior} = \frac{
\text{Probability of the data} \times \text{Prior}
}{
\text{Average probability of the data}
}
$$
```{r WWW, eval=TRUE}

library(tidyverse)

# set number of points for the grid 
n <- 10

# create a grid approximation tableT
www <- 
    tibble(
        # define grid
        p = seq(0, 1, length.out = n),
        # compute prior at each parameter value
        prior = 1
    ) %>% 
    mutate(
        # compute the likelihood at each parameter value
        plausibility = dbinom(3, size = 3, prob = p),
        # comput the unstandardised posterior
        unstd_post = plausibility * prior,
        # standardise the posterior
        posterior = unstd_post / sum(unstd_post)
    )

```

```{r}

grid_plot <- function(post_df, title_text) {
post_df %>% 
    ggplot(
        aes(
            x = p,
            y = posterior
        )
    ) +
    geom_line(alpha = 0.4) +
    geom_point(alpha = 0.8) + 
    labs(title = title_text) + 
        ggthemes::theme_tufte()
    
}



```

It makes sense that for `www` observation, we have a monotonically increasing posterior. Higher values of $p$ are more likely, given we have _only_ observed water.


```{r}
grid_plot(www, "w w w")
```

Now for the second set of observations, `wwwl`. This should be slightly more interesting, as there is one land observation, so we know with certainty that $p \neq 1$, whereas in `www`, this was the most plausible value for $p$.

```{r}

wwwl <- 
    tibble(
        # define grid
        p = seq(0, 1, length.out = n),
        # compute prior at each parameter value
        prior = 1
    ) %>% 
    mutate(
        # compute the likelihood at each parameter value
        plausibility = dbinom(3, size = 4, prob = p),
        # comput the unstandardised posterior
        unstd_post = plausibility * prior,
        # standardise the posterior
        posterior = unstd_post / sum(unstd_post)
    )

grid_plot(wwwl, "w w w l")

```

```{r}
lwwlwww <- 
    tibble(
        # define grid
        p = seq(0, 1, length.out = n),
        # compute prior at each parameter value
        prior = 1
    ) %>% 
    mutate(
        # compute the likelihood at each parameter value
        plausibility = dbinom(5, size = 7, prob = p),
        # comput the unstandardised posterior
        unstd_post = plausibility * prior,
        # standardise the posterior
        posterior = unstd_post / sum(unstd_post)
    )

grid_plot(lwwlwww, "l w w l w w")


```

### 2M2

We now assume a prior $P: [0,1] \to \{0, \theta\}$ such that 
$$
p \mapsto 
\begin{cases}
0 & p < 0.5\\
\theta & p \geqslant 0.5
\end{cases}
$$
where $\theta \in \mathbb R^+$, and again compute the grid approximations as in 2M1.

Now I've got the general idea of each line of the grid approximation, I'll now functionalise it. 

```{r grid approx}
grid_theta <- function(w, sample_size) {
  # get a random positive value
  theta <- runif(1, 1, 10000)
  
  # return tibble of grid approx
     tibble(
        # define grid
        p = seq(0, 1, length.out = n),
        # compute prior at each parameter value
        prior = if_else(p < 0.5, 0, theta)
    ) %>% 
    mutate(
        # compute the likelihood at each parameter value
        plausibility = dbinom(w, size = sample_size, prob = p),
        # comput the unstandardised posterior
        unstd_post = plausibility * prior,
        # standardise the posterior
        posterior = unstd_post / sum(unstd_post)
    )
}
  
```

```{r}
# www

grid_theta(3, 3) %>% 
  grid_plot("w w w, prior = 0 for p < 0.5")

```
```{r}
# wwwl

grid_theta(3, 4) %>% 
  grid_plot("w w w l, prior = 0 for p < 0.5")

```

```{r}
# lwwlwww
grid_theta(5, 7) %>% 
  grid_plot("l w w l w w w, prior = 0 for p < 0.5")


```

Since 
$$
\text{Posterior} = \frac{
\text{Probability of the data} \times \text{Prior}
}{
\text{Average probability of the data}
}
$$
In all three questions, for $p< 0.5$, the posterior is 0.

Since the numerator is a product of the probability of the data and the prior, which is set to 0 for $p < 0.5$, and any product containing zero is zero, we have a posterior of 0 for all values $p < 0.5$. That is, where the prior is 0, this will *always* produce a posterior of 0, regardless of the value of the probability of the data.  

## hard