
@article{wilson_good_2017,
	title = {Good enough practices in scientific computing},
	volume = {13},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1005510},
	doi = {10.1371/journal.pcbi.1005510},
	language = {English},
	number = {6},
	urldate = {2018-11-17},
	journal = {PLOS Computational Biology},
	author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
	editor = {Ouellette, Francis},
	month = jun,
	year = {2017},
	keywords = {dumpsterfire, measured.},
	pages = {e1005510},
	file = {Wilson et al. - 2017 - Good enough practices in scientific computing.pdf:/home/charles/Zotero/storage/DTSH3KHG/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf:application/pdf}
}

@misc{wickham_testthat_2011,
	title = {testthat: {Get} {Started} with {Testing}},
	abstract = {Software testing is important, but many of us don’t do it because it is frustrating and boring. testthat is a new testing framework for R that is easy learn and use, and integrates with your existing workﬂow. This paper shows how, with illustrations from existing packages.},
	author = {Wickham, Hadley},
	year = {2011},
	file = {Wickham - 2011 - testthat Get Started with Testing.pdf:/home/charles/Zotero/storage/VURNBAVZ/Wickham - 2011 - testthat Get Started with Testing.pdf:application/pdf}
}

@misc{marwick_rrtools_2018,
	title = {rrtools: {Creates} a reproducible research compendium},
	url = {https://github.com/benmarwick/rrtools},
	author = {Marwick, Ben},
	year = {2018},
	keywords = {measured.}
}

@article{wilson_best_2014,
	title = {Best {Practices} for {Scientific} {Computing}},
	volume = {12},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.1001745},
	doi = {10.1371/journal.pbio.1001745},
	language = {English},
	number = {1},
	urldate = {2018-11-10},
	journal = {PLoS Biology},
	author = {Wilson, Greg and Aruliah, D. A. and Brown, C. Titus and Chue Hong, Neil P. and Davis, Matt and Guy, Richard T. and Haddock, Steven H. D. and Huff, Kathryn D. and Mitchell, Ian M. and Plumbley, Mark D. and Waugh, Ben and White, Ethan P. and Wilson, Paul},
	editor = {Eisen, Jonathan A.},
	month = jan,
	year = {2014},
	keywords = {dumpsterfire, measured.},
	pages = {e1001745},
	file = {Wilson et al. - 2014 - Best Practices for Scientific Computing.pdf:/home/charles/Zotero/storage/3XNS9PYQ/Wilson et al. - 2014 - Best Practices for Scientific Computing.pdf:application/pdf}
}

@misc{hester_covr_2018,
	title = {covr: {Test} {Coverage} for {Packages}},
	url = {https://CRAN.R-project.org/package=covr},
	author = {Hester, Jim},
	year = {2018},
	keywords = {measured.}
}

@article{parker_opinionated_2017,
	title = {Opinionated analysis development},
	doi = {10.7287/peerj.preprints.3210v1},
	abstract = {Traditionally, statistical training has focused primarily on mathematical derivations and proofs of statistical tests. The process of developing the technical artifact—that is, the paper, dashboard, or other deliverable—is much less frequently taught, presumably because of an aversion to cookbookery or prescribing specific software choices. In this paper I argue that it’s critical to teach analysts how to go about developing an analysis in order to maximize the probability that their analysis is reproducible, accurate, and collaborative. A critical component of this is adopting a blameless postmortem culture. By encouraging the use of and fluency in tooling that implements these opinions, as well as a blameless way of correcting course as analysts encounter errors, we as a community can foster the growth of processes that fail the practitioners as infrequently as possible.},
	language = {English},
	urldate = {2018-11-03},
	journal = {preprint},
	author = {Parker, Hilary},
	year = {2017},
	keywords = {dumpsterfire, measured.},
	file = {Parker - Opinionated analysis development.pdf:/home/charles/Zotero/storage/U5MM7QUF/Parker - Opinionated analysis development.pdf:application/pdf}
}

@misc{katz_super_2019,
	title = {Super {RSEs}: {Combining} research and service in three dimensions of {Research} {Software} {Engineering}},
	shorttitle = {Super {RSEs}},
	url = {https://danielskatzblog.wordpress.com/2019/07/12/},
	abstract = {We typically think of Research Software Engineers (RSEs) as working to support one or more researchers, either one-on-one or through a university’s centra…},
	language = {en},
	urldate = {2019-07-16},
	journal = {Daniel S. Katz's blog},
	author = {Katz, Daniel S. and McHenry, Kenton},
	month = jul,
	year = {2019},
	file = {Snapshot:/home/charles/Zotero/storage/RQQ9Y9FE/amp.html:text/html}
}

@article{gray_truth_2019,
	title = {Truth, {Proof}, and {Reproducibility}: {There}'s no counter-attack for the codeless},
	copyright = {All rights reserved},
	shorttitle = {Truth, {Proof}, and {Reproducibility}},
	url = {http://arxiv.org/abs/1907.05947},
	abstract = {Current concerns about reproducibility in many research communities can be traced back to a high value placed on empirical reproducibility of the physical details of scientific experiments and observations. For example, the detailed descriptions by 17th century scientist Robert Boyle of his vacuum pump experiments are often held to be the ideal of reproducibility as a cornerstone of scientific practice. Victoria Stodden has claimed that the computer is an analog for Boyle's pump -- another kind of scientific instrument that needs detailed descriptions of how it generates results. In the place of Boyle's hand-written notes, we now expect code in open source programming languages to be available to enable others to reproduce and extend computational experiments. In this paper we show that there is another genealogy for reproducibility, starting at least from Euclid, in the production of proofs in mathematics. Proofs have a distinctive quality of being necessarily reproducible, and are the cornerstone of mathematical science. However, the task of the modern mathematical scientist has drifted from that of blackboard rhetorician, where the craft of proof reigned, to a scientific workflow that now more closely resembles that of an experimental scientist. So, what is proof in modern mathematics? And, if proof is unattainable in other fields, what is due scientific diligence in a computational experimental environment? How do we measure truth in the context of uncertainty? Adopting a manner of Lakatosian conversant conjecture between two mathematicians, we examine how proof informs our practice of computational statistical inquiry. We propose that a reorientation of mathematical science is necessary so that its reproducibility can be readily assessed.},
	urldate = {2019-07-17},
	journal = {arXiv:1907.05947 [math]},
	author = {Gray, Charles T. and Marwick, Ben},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.05947},
	keywords = {06-06, Mathematics - History and Overview},
	annote = {Comment: To be presented at the upcoming research school in statistics https://sites.google.com/view/rssds2019/home},
	file = {arXiv\:1907.05947 PDF:/home/charles/Zotero/storage/3Y65QUBV/Gray and Marwick - 2019 - Truth, Proof, and Reproducibility There's no coun.pdf:application/pdf;arXiv.org Snapshot:/home/charles/Zotero/storage/W9WPV9F9/1907.html:text/html}
}

@book{bryan_happy_2019,
	title = {Happy {Git} with {R}},
	url = {https://happygitwithr.com/resources.html},
	author = {Bryan, Jennifer},
	year = {2019}
}

@misc{wickham_usethis_2019,
	title = {usethis: {Automate} {Package} and {Project} {Setup}},
	url = {https://CRAN.R-project.org/package=usethis},
	author = {Wickham, Hadley and Bryan, Jennifer},
	year = {2019},
	annote = {R package version 1.5.1}
}
