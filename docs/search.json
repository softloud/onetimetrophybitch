{
  "articles": [
    {
      "path": "about.html",
      "title": "trophybitch",
      "description": "a one-time trophy bitch screams into the void\n",
      "author": [],
      "contents": "\npublications\nSee references at bottom of this page for\ncitation details.\ntitle\ndiscipline\ndescription\ncode::proof:\nPrepare for most weather conditions(Gray\n2019)\nmetaprogramming\nsecond paper from thesis, accepted to springer computer science\nproceedings something something, metaprogramming\nTruth, Proof, and\nReproducibility: There’s no counter-attack for the codeless\nmetamathematics\nfirst paper from current thesis has been accepted into springer\ncomputer proceedings something something, metamathematics(Gray and Marwick\n2019)\nThe\nHomomorphism Lattice Induced by a Finite Algebra\nabstract algebra\nThe publication of my honours (masters-level) thesis in abstract\nalgebra. Authorship is alphabetical and equal in mathematics. Brian and\nJane did a great deal of work on the final paper. But in my thesis, the\nproofs were all (but two small introductory proofs Brian wrote for\ncompleteness) my own. (Davey, Gray,\nand Pitkethly 2018)\nProfiling for Profit\nmarketing\noddly enough, an award-winning consumer behaviour paper, best in the\nbusiness ethics track (Harrison and Gray\n2010; Harrison and Gray 2012)\neviatlas::\necology\nwrote one paragraph and contributed to discussion on this ecology\nproject(Haddaway et al.\n2019)\ntodo:look up\npsychology\nspent two deep-dive days conjuring a package out of an equation for\nthe Misinformation Lab, and two co-authored papers are submitted\nCoding of\nKindness(Gray et al.\n2019)\nopen science\na rephrasing of the Code of Conduct, which did not win over the\npeople I was trying to communicate with, but did mean that other\nsurvivors reached out, and that was quite lovely\nsome other cool shit i did 2\nMost recent first.\nreprofail seminar - long\nversion\nreadme\nfor talk with abstract\n\n\nheidelberg laureate forum\nIt was a delight to participate in the opening of the Heidelberg Laureate\nForum, where the laureates of mathematics and computer science meet\nthe next generation.\nI’ve cued this video to my presentation on the algebra of\ncomposition.\n\n\nThere’s no Nobel prize in mathematics and computer science. Instead,\nthere is the Turing award, the Fields medal, and the Abel prize. Each\nyear the Heidelberg Laureate Forum Foundation gathers these laureates of\nmathematics and computing, along with two hundred talented early-career\nresearchers, and science writers, for a week of lectures, workshops, and\nevents designed to foster conversation and ideas.\nIt was a gift to present at the opening ceremony and have that\nintroduction; I wasn’t in want of conversation for the entire event. I\ngot to meet so many interesting people.\nSlides for the presentation can be found here.\ncode like a girl tour\ncheck out twitter\nreel from the tour\nand the event\n\nuseR! workshop\n\n\nmathbassador gray\n\n\n\n\n\nDavey, Brian A., Charles T. Gray, and Jane G. Pitkethly. 2018.\n“The Homomorphism Lattice Induced by a Finite\nAlgebra.” Order 35 (2): 193–214. https://doi.org/10.1007/s11083-017-9426-3.\n\n\nGray, Charles T. 2019. “: Prepare for\nMost Weather Conditions.” https://arxiv.org/abs/1910.06964.\n\n\nGray, Charles T., and Ben Marwick. 2019. “Truth,\nProof, and Reproducibility:\nThere’s No Counter-Attack for the Codeless.”\narXiv:1907.05947 [Math], July. https://arxiv.org/abs/1907.05947.\n\n\nGray, Charles T., Ben Marwick, Jen Richmond, Emily Kothe, Mathew Ling,\nand Brian Knaus. 2019. “Coding of Kindness:\nLet’s Figure Out How to Be Better Humans to Each\nOther.” Coding of Kindness.\nhttps://softloud.github.io/codingofkindness/.\n\n\nHaddaway, Neal R., Andrew Feierman, Matthew J. Grainger, Charles T.\nGray, Ezgi Tanriver-Ayder, Sanita Dhaubanjar, and Martin J. Westgate.\n2019. “EviAtlas: A Tool for Visualising Evidence\nSynthesis Databases.” Environmental Evidence 8 (1): 22.\nhttps://doi.org/10.1186/s13750-019-0167-1.\n\n\nHarrison, Paul, and Charles Gray. 2010. “The Ethical and Policy\nImplications of Profiling ‘Vulnerable’ Customers:\nImplications of Customer Profiling.”\nInternational Journal of Consumer Studies 34 (4): 437–42. https://doi.org/10.1111/j.1470-6431.2010.00873.x.\n\n\nHarrison, Paul, and Charles Ti Gray. 2012. Profiling for Profit : A\nReport on Target Marketing and Profiling Practices in the Credit\nIndustry. Consumer Action Law Centre.\n\n\n\n\n",
      "last_modified": "2023-03-30T10:26:09+02:00"
    },
    {
      "path": "chapter-2.html",
      "title": "chapter 2",
      "description": "exercises from chapter 2 statistical rethinking\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\neasy\n2E1\n2E2\n2E3\n2E4\n\nmedium\n2M1\n2M2\n2M3\n2M4\n2M5\n2M6\n2M7\n\nhard\n2H1\n2H2\n2H3\n\n\n\n\n# packages used\nlibrary(tidyverse)\nlibrary(gt)\n\n\nChecking solutions with this helpful\ndiscussion and this other set\nof solutions. Yay for open learning.\n\n\n# pandan::pandan_view(project = \"rethinking\", distill = TRUE, update = FALSE)\n\n\neasy\n2E1\n\nWhich of the expressions below correspond to the statement: the\nprobability of rain on Monday?\n\nP(rain)\nP(rain|Monday)\nP(Monday|rain)\nP(rain, Monday) / P(Monday)\nAdapting the equation on p. 37 to a more generalised statement, for a\nparameter of interest \\(\\theta\\) and\nobservations \\(y\\) dependent on that\nparameter, we have\n\\[\nP(\\theta | y) = \\frac{P(y|\\theta)P(\\theta)}{P(y)} = P(\\theta, y)/P(y)\n\\]\nThen P(rain|Monday) = P(rain, Monday) / P(Monday).\nSo, I conclude that 2. and 4. correspond to the statement.\n2E2\nThe following statement corresponds to the expression\nP(Monday|rain):\nThe probability that it is Monday, given that it is raining.\n2E3\n\nThe probability that it is Monday given that it is raining.\n\nCan be expressed\nP(Monday|rain) = P(rain|Monday)P(Monday)/P(rain)\nSo, the following statements correspond.\nP(Monday|rain) and 4. P(rain|Monday)P(Monday)/P(rain)\n2E4\n\nThe Bayesian statistician Finetti began his book on probablity theory\nwith the declaration: “PROBABILITY DOES NOT EXIST”. What he meant is\nthat probability is a device for describing uncertainty from the\nperspective ofg an observer with limited knowledge; it has no objective\nreality. What does it mean to say, “the probability of water is\n0.7”?\n\nBased on the observations we have of water and land from globe\ntossing, the most credible proportion of the globe is approximately\n0.7.\nmedium\n2M1\n\nCompute and plot the grid approximate posterior distribution for each\nof the following sets of observations. In each case, assume a uniform\nprior for p.\n\nWWW\nWWWL\nLWWLWWW\nWe assume \\[\n\\begin{array}\n\\\\W & \\sim & \\text{binomial}(N, p)\\\\\np & \\sim & \\text{uniform}(0, 1)\n\\end{array}\n\\] where \\(W\\) is the number of\nwater observations and \\(N = L + W\\) is\nthe total number of water and land observations. We only know \\(p\\) is a proportion, so we know that it\nfalls in \\([0,1]\\).\nWe are interested in finding \\[\nP(p|W,L) = P(W,L|p)P(p)/P(W,L)\n\\] where \\(P(p|W,L)\\) is the\nposterior, \\(P(W,L|p)\\) is the\nprobability of the data, \\(P(W,L)\\) is\nthe prior.\n\nAnd this is Bayes’ theorem. It says the the probability of any\nparticular value of \\(p\\), considering\nthe data is equal to the product of the relative plausibility of the\ndata, conditional on \\(p\\), and the\nprior plausibility of \\(p\\), divided by\nthe average probability of the data.\n\n\\[\n\\text{Posterior} = \\frac{\n\\text{Probability of the data} \\times \\text{Prior}\n}{\n\\text{Average probability of the data}\n}\n\\]\n\n\nlibrary(tidyverse)\n\n# set number of points for the grid \nn <- 10\n\n# create a grid approximation tableT\nwww <- \n    tibble(\n        # define grid\n        p = seq(0, 1, length.out = n),\n        # compute prior at each parameter value\n        prior = 1\n    ) %>% \n    mutate(\n        # compute the likelihood at each parameter value\n        plausibility = dbinom(3, size = 3, prob = p),\n        # comput the unstandardised posterior\n        unstd_post = plausibility * prior,\n        # standardise the posterior\n        posterior = unstd_post / sum(unstd_post)\n    )\n\n\n\n\ngrid_plot <- function(post_df, title_text) {\npost_df %>% \n    ggplot(\n        aes(\n            x = p,\n            y = posterior\n        )\n    ) +\n    geom_line(alpha = 0.4) +\n    geom_point(alpha = 0.8) + \n    labs(title = title_text) + \n        ggthemes::theme_tufte()\n    \n}\n\n\nIt makes sense that for www observation, we have a\nmonotonically increasing posterior. Higher values of \\(p\\) are more likely, given we have\nonly observed water.\n\n\ngrid_plot(www, \"w w w\")\n\n\n\nNow for the second set of observations, wwwl. This\nshould be slightly more interesting, as there is one land observation,\nso we know with certainty that \\(p \\neq\n1\\), whereas in www, this was the most plausible\nvalue for \\(p\\).\n\n\nwwwl <- \n    tibble(\n        # define grid\n        p = seq(0, 1, length.out = n),\n        # compute prior at each parameter value\n        prior = 1\n    ) %>% \n    mutate(\n        # compute the likelihood at each parameter value\n        plausibility = dbinom(3, size = 4, prob = p),\n        # comput the unstandardised posterior\n        unstd_post = plausibility * prior,\n        # standardise the posterior\n        posterior = unstd_post / sum(unstd_post)\n    )\n\ngrid_plot(wwwl, \"w w w l\")\n\n\n\n\n\nlwwlwww <- \n    tibble(\n        # define grid\n        p = seq(0, 1, length.out = n),\n        # compute prior at each parameter value\n        prior = 1\n    ) %>% \n    mutate(\n        # compute the likelihood at each parameter value\n        plausibility = dbinom(5, size = 7, prob = p),\n        # comput the unstandardised posterior\n        unstd_post = plausibility * prior,\n        # standardise the posterior\n        posterior = unstd_post / sum(unstd_post)\n    )\n\ngrid_plot(lwwlwww, \"l w w l w w\")\n\n\n\n2M2\nWe now assume a prior \\(P: [0,1] \\to \\{0,\n\\theta\\}\\) such that \\[\np \\mapsto\n\\begin{cases}\n0 & p < 0.5\\\\\n\\theta & p \\geqslant 0.5\n\\end{cases}\n\\] where \\(\\theta \\in \\mathbb\nR^+\\), and again compute the grid approximations as in 2M1.\nNow I’ve got the general idea of each line of the grid approximation,\nI’ll now functionalise it.\n\n\ngrid_theta <- function(w, sample_size) {\n  # get a random positive value\n  theta <- runif(1, 1, 10000)\n  \n  # return tibble of grid approx\n     tibble(\n        # define grid\n        p = seq(0, 1, length.out = n),\n        # compute prior at each parameter value\n        prior = if_else(p < 0.5, 0, theta)\n    ) %>% \n    mutate(\n        # compute the likelihood at each parameter value\n        plausibility = dbinom(w, size = sample_size, prob = p),\n        # comput the unstandardised posterior\n        unstd_post = plausibility * prior,\n        # standardise the posterior\n        posterior = unstd_post / sum(unstd_post)\n    )\n}\n\n\n\n\n# www\n\ngrid_theta(3, 3) %>% \n  grid_plot(\"w w w, prior = 0 for p < 0.5\")\n\n\n\n\n\n# wwwl\n\ngrid_theta(3, 4) %>% \n  grid_plot(\"w w w l, prior = 0 for p < 0.5\")\n\n\n\n\n\n# lwwlwww\ngrid_theta(5, 7) %>% \n  grid_plot(\"l w w l w w w, prior = 0 for p < 0.5\")\n\n\n\nSince \\[\n\\text{Posterior} = \\frac{\n\\text{Probability of the data} \\times \\text{Prior}\n}{\n\\text{Average probability of the data}\n}\n\\] In all three questions, for \\(p<\n0.5\\), the posterior is 0.\nSince the numerator is a product of the probability of the data and\nthe prior, which is set to 0 for \\(p <\n0.5\\), and any product containing zero is zero, we have a\nposterior of 0 for all values \\(p <\n0.5\\). That is, where the prior is 0, this will always\nproduce a posterior of 0, regardless of the value of the probability of\nthe data.\n2M3\n\nSuppose there are two globes, one for Earth and one for Mars. The\nEarth globe is 70% covered in water. The Mars globe is 100% land.\n\nSo, P(water|Earth) = 0.7 and P(land|Mars) = 1.0.\n\nFurther suppose that one of these globes - you don’t know which - was\ntossed in the air and produced a land observation. Assume each globe was\nequally likely to be tossed.\n\nSo, P(Earth) = 0.5 and P(Mars) = 0.5.\n\nShow that the posterior probability that the globe was Earth\nconditional on seeing land is 0.23, that is, P(Earth|land) = 0.23.\n\nWe wish to show that P(Earth|land) = 0.23.\nWell, Bayes’ theorem gives us,\nP(Earth|land) = P(land|Earth)P(Earth)/P(land).\nSo we need to find P(land|Earth), P(Earth), and P(land).\nWe know P(Earth) = 0.5.\nNow P(water|Earth) = 0.7, so P(land|Earth) = 0.3, since there are\nonly two possibilities on Earth.\nFinally, for P(land), we note that, going back to the original\ngeneralised notation, \\[\nP(y) = E(P(y|\\theta)) = \\int P(y|\\theta)P(\\theta)d\\theta\n\\] so, since in this example we are dealing with discrete\npossibilities,\nP(land) = E(P(land|Planet)) = \\(\\displaystyle\\sum_{\\text{Planet } \\in\n\\{\\text{Earth},\\text{Mars}\\}}\\) P(land|Planet)P(planet) =\nP(land|Earth) P(Earth) + P(land|Mars) P(Mars).\nThen P(land) is\n\n\n0.3 * 0.5 + 1 * 0.5\n\n[1] 0.65\n\nSo, we have P(land|Earth), by Bayes’ theorem, is\n\n\n(0.3 * 0.5)/(0.3 * 0.5 + 1 * 0.5)\n\n[1] 0.2307692\n\n2M4\n\nSuppose you have a deck with only three cards.\n\nLet \\(X\\) denote the set of three\ncards, so \\(|X|=3\\).\n\nAnd each side is either black or white. One card has two black sides.\nThe second card has one black and one white side. The third card has two\nwhite sides.\n\nSo, we could say \\(X = \\{BB, BW,\nWW\\}\\).\n\nSomeone pulls out a card and places it flat on a table. A black side\nis shown facing up, but you don’t know the colour of the side facing\ndown.\n\nLet \\(y\\) denote the observation\n\\(B\\). Now, clearly \\(P(B|WW) = 0\\).\n\nUse the counting method from Section 2. This means counting up the\nways that each card could produce the observed data.\n\n\nShow that the probability that the other side is also black is\n2/3.\n\nWe wish to show that \\(P(BB|B) =\n2/3\\).\n\n\ntibble(\n  # possible cards\n  conjecture = c(\"BB\", \"BW\", \"WW\"),\n  # probability of each card\n  p = rep(1 / 3, 3),\n  # ways the card can produce the observation B\n  ways = c(2, 1, 0)\n) %>%\n  # calculate the plausibility\n  mutate(prod = p * ways,\n         plausibility = prod / sum(prod)) %>%\n  gt() %>%\n  cols_label(\n    conjecture = \"possible cards\",\n    p = \"likelihood of drawing card\",\n    ways = \"ways card can produce B\",\n    prod = \"p x ways\",\n    plausibility = \"plausbility - p x ways/sum(p x ways)\"\n  ) %>% \n  data_color(\n    columns = vars(plausibility),\n    colors = scales::col_bin(\n      palette = c(\"#93a1a1\", \"white\"),\n      bins = 2,\n      domain = NULL,\n      reverse = TRUE\n    )\n  ) %>% \n  data_color(\n    columns = vars(conjecture),\n    colors = scales::col_factor(\n      palette = c(\"#93a1a1\", \"white\", \"white\"),\n      domain = NULL\n    )\n  )\n\n\npossible cards\n      likelihood of drawing card\n      ways card can produce B\n      p x ways\n      plausbility - p x ways/sum(p x ways)\n    BB\n0.3333333\n2\n0.6666667\n0.6666667BW\n0.3333333\n1\n0.3333333\n0.3333333WW\n0.3333333\n0\n0.0000000\n0.0000000\n\n2M5\nNow suppose there are four cards \\(\\{BB,\nBW, WW, BB\\}\\).\nAgain calculate the probability the other side is black.\nThis time I’ll use TeX so I can see how I worked it on paper.\n\\[\n\\begin{array}{cccc}\n\\text{possible card} & \\text{chance of card} & \\text{ways card\nproduces B} & \\text{chance} \\times \\text{ways} &\n\\text{plausibility (product / $\\Sigma$)}\\\\\n\\hline\nBB & 1/2 & 2 & 1 & 4/5\\\\\nBW & 1/4 & 1 & 1/4 & 1/5\\\\\nWW & 1/4 & 0 & 0 & 0\\\\\n\\hline\n&&&\\Sigma = 5/4\n\\end{array}\n\\]\nSo, the probability the other side is black is\n\n\n4/5\n\n[1] 0.8\n\n2M6\n\nImagine the black ink is heavy, and so cards with black sides are\nheavier than cards with white sides.\n\nSo, P(B) < P(W).\n\nAgain assume there are three cards \\(\\{BB,\nBW, WW\\}\\). After experimenting a number of times, you conclude\nthat for every way to pull the \\(BB\\)\ncard from the bag, there are 2 ways to pull the BW card, and 3 ways to\npull the WW card. Again suppose that a card is pulled and a black side\nappears face up. Show that the probability the other side is black is\nnow 0.5. Use the counting method, as before.\n\n2M7\n\nAssume againt he original card problem, with a single card showing a\nblack side face up. Before looking at the other side, we draw another\ncard from the bag and lay it face up on the table. The face that is\nshown on the new card is white. Show that the probability that the first\ncard, the one showing a black side, has black on its other side is now\n0.45. Using the counting method, if you can. Hint: Treat this like the\nsequence of globe tosses, counting all the ways to see each observation,\nfor each possible first card.\n\nhard\n2H1\n\nSuppose there are two species of panda bear. Both are equally common\nin the wild and live int he sampe places. The look exactly alide and eat\nthe same food, and there is yet no geneetic assay capable of telling\nthem apart. They differ however in their family sizes. Specias A gives\nbith to twins 10% of the time, otherwise birthing a single infant.\nSpecies B births twins 20% of the time, otherwise birhting singleton\ninfants. Assume these numbers are known with certainty, from many years\nof field research.\n\n\nNow suppose you are managing a captive panda breeding program. You\nhave a new female panda of unknown species, and she has just given birth\nto twins. What is the probability that her next birth will also be\ntwins?\n\n2H2\n\nRecall all the facts from the problem above.\n\n\nNow compute the probability that the panda we have is from species A,\nassuming we have observed only the first birth and that it was\ntwins.\n\n2H3\n\nContinuing on from the previous problem, suppose the same panda\nmother has a second birth and that it is not twins, but a singleton\ninfant.\n\n\nCompute the posterior probability that this panda is species A.\n\n\n\n\n",
      "last_modified": "2023-03-30T10:26:09+02:00"
    },
    {
      "path": "index.html",
      "title": "measured.",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2023-03-30T14:29:16+02:00"
    },
    {
      "path": "README.html",
      "author": [],
      "contents": "\nonetimetrophybitch\n\n\n",
      "last_modified": "2023-03-30T14:29:16+02:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
