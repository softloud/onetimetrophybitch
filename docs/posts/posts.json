[
  {
    "path": "posts/2021-03-29-lists/",
    "title": "list and unlist",
    "description": "In which I continue to be confused about ::unlist, but manage to join two lists, as required",
    "author": [
      {
        "name": "Charles T. Gray",
        "url": "https://softloud.github.io/onetimetrophybitch/about.html"
      }
    ],
    "date": "2021-03-29",
    "categories": [
      "codeflow"
    ],
    "contents": "\n\n\n# pkgs used\nlibrary(tidyverse)\n\n\n\n\n\nfirst_list <-\n  list(\n    a = letters[1:3],\n    b = \"mittens\",\n    c = c(TRUE, FALSE),\n    d = rnorm(4)\n  )\n\nsecond_list <- \n  list(\n    e = \"buttons\",\n    f = letters[1:5],\n    g = runif(2)\n  )\n  \n\n# if I have a list(list, list) how do I get list of the elmenets of hte list? \n# Elements of first list as elements,\n# and elements of second list as elements?\n\nunlist(first_list, second_list) %>% \n  dim()\n\n\nNULL\n\n# huh dim?\n\nunlist(first_list, second_list) %>% \n  class()\n\n\n[1] \"character\"\n\n# character?!\n\nunlist(first_list, second_list)\n\n\n                  a1                   a2                   a3 \n                 \"a\"                  \"b\"                  \"c\" \n                   b                   c1                   c2 \n           \"mittens\"               \"TRUE\"              \"FALSE\" \n                  d1                   d2                   d3 \n\"-0.658853976666822\"  \"-1.29571869155379\" \"-0.541580614619811\" \n                  d4 \n\"-0.150846740435645\" \n\n# oh it made everything flat, that's not what I want. I still want the elements \n# to remain lists, what about this recursive argument?\n\nunlist(first_list, second_list, recursive = FALSE) \n\n\n                  a1                   a2                   a3 \n                 \"a\"                  \"b\"                  \"c\" \n                   b                   c1                   c2 \n           \"mittens\"               \"TRUE\"              \"FALSE\" \n                  d1                   d2                   d3 \n\"-0.658853976666822\"  \"-1.29571869155379\" \"-0.541580614619811\" \n                  d4 \n\"-0.150846740435645\" \n\n# okay, maybe I'm overthinking this, what happens if I make a list of two lists\nlist(first_list, second_list)\n\n\n[[1]]\n[[1]]$a\n[1] \"a\" \"b\" \"c\"\n\n[[1]]$b\n[1] \"mittens\"\n\n[[1]]$c\n[1]  TRUE FALSE\n\n[[1]]$d\n[1] -0.6588540 -1.2957187 -0.5415806 -0.1508467\n\n\n[[2]]\n[[2]]$e\n[1] \"buttons\"\n\n[[2]]$f\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n[[2]]$g\n[1] 0.08550033 0.32802539\n\n# ugh, no I think I ended up with a list of 2 lists\nlist(first_list, second_list) %>% \n  str()\n\n\nList of 2\n $ :List of 4\n  ..$ a: chr [1:3] \"a\" \"b\" \"c\"\n  ..$ b: chr \"mittens\"\n  ..$ c: logi [1:2] TRUE FALSE\n  ..$ d: num [1:4] -0.659 -1.296 -0.542 -0.151\n $ :List of 3\n  ..$ e: chr \"buttons\"\n  ..$ f: chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n  ..$ g: num [1:2] 0.0855 0.328\n\n# see I have a list of 4 and a list of 3, I want a list of 7, not a list of 2 \n# comprising 4 and 3 ugh\n\n# can I use map to extract the elements?\nmap(first_list, 1)\n\n\n$a\n[1] \"a\"\n\n$b\n[1] \"mittens\"\n\n$c\n[1] TRUE\n\n$d\n[1] -0.658854\n\n# how is this different from first list?\nfirst_list\n\n\n$a\n[1] \"a\" \"b\" \"c\"\n\n$b\n[1] \"mittens\"\n\n$c\n[1]  TRUE FALSE\n\n$d\n[1] -0.6588540 -1.2957187 -0.5415806 -0.1508467\n\n# oh that's not what I want\n# fuck me, I'm going to look this up on the tubes now.\n\n\n\nAnd switch to a post. This is proving to be more involved than I thought.\nThis post on stackoverflow suggests using ::append, despite the function saying it is for vectors. Worth a try.\n\n\nappend(first_list, second_list)\n\n\n$a\n[1] \"a\" \"b\" \"c\"\n\n$b\n[1] \"mittens\"\n\n$c\n[1]  TRUE FALSE\n\n$d\n[1] -0.6588540 -1.2957187 -0.5415806 -0.1508467\n\n$e\n[1] \"buttons\"\n\n$f\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n$g\n[1] 0.08550033 0.32802539\n\n# compare with \nfirst_list\n\n\n$a\n[1] \"a\" \"b\" \"c\"\n\n$b\n[1] \"mittens\"\n\n$c\n[1]  TRUE FALSE\n\n$d\n[1] -0.6588540 -1.2957187 -0.5415806 -0.1508467\n\nsecond_list\n\n\n$e\n[1] \"buttons\"\n\n$f\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n$g\n[1] 0.08550033 0.32802539\n\nHuh, well, there you go. So a base function to the rescue.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-29T20:01:13+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-09-06-nma/",
    "title": "nma",
    "description": "Work in progress.",
    "author": [
      {
        "name": "Charles T. Gray",
        "url": "https://github.com/softloud/onetimetrophybitch"
      }
    ],
    "date": "2020-09-06",
    "categories": [],
    "contents": "\nTable of Contents\nnetwork meta-analysis\nthinking through the theory\nlinear regression\nmeta-regression\nBayesian network meta-analysis\n\nexample dataset: parkinsons_dat\nnetwork meta-analysis\ncochrane reporting\nevidence base and network structure\nflow of evidence\ncontribution matrix\n\nresults\nintervention effects\nrankograms\n\n\nthreshold analysis\nthe math of ::nma_thresh\nnmathresh:: example: social anxiety\nreferences\n\n\n# last updated\nSys.Date()\n\n[1] \"2020-10-07\"\n\n\n\n# packages used in this blogpost\n\n# general purpose\nlibrary(tidyverse) \nlibrary(kableExtra) # tables\nlibrary(skimr) # summary\nlibrary(patchwork) # vis + vis\n\n# toolchain walkthrough\nlibrary(multinma) # network meta-analysis\nlibrary(nmathresh) # threshold analysis\nlibrary(metafor) # for (non network) meta-analysis\n\n# special purpose\nlibrary(fdrtool) # for halfnormal distribution\nlibrary(simeta) # borrowing some handy functions \n# note: should port these to my helper functions pkg\n\n# for reproducibility\nset.seed(40)\n\nnetwork meta-analysis\nNetwork meta-analysis is a statistical method for aggregating multiple pairwise comparisons on a response of interest (Higgins et al. 2019). For example, perhaps three studies propose different treatments for a particular medical condition, network meta-analysis enables us to directly and indirectly compare the treatments’ effects. Network meta-analysis is a technique for identifying which treatments are more effective than others, rather than whether a treatment is effective (Harrer et al. 2019).\nIn this toolchain walkthrough, an opinionated documentation of a scientific workflow (Gray 2019), I computationally step through the packages multinma:: (Phillippo 2020) and nnathresh (Phillippo et al. 2018) for a network meta-analysis with threshold analysis.\nthinking through the theory\nBayesian network meta-analysis is a form of meta-analysis, and meta-analysis is form of linear regression. We solve for a line of best fit. How do we describe this particular line?\nlinear regression\n\\[\ny \\sim \\text{normal}(BX, \\sigma^2)\n\\]\nmeta-regression\nFor the \\(k\\)th study’s effect, for a random effects model, we assume\n\\[\ny_k \\sim \\text{normal}(\\mu + u_k, \\sigma_k^2)\\\\\nu_k \\sim \\text{normal}(0, \\tau^2)\n\\]\nwhere \\(\\mu\\) represents the effect of interest and \\(u_k\\) the heterogeneity of the studies, with spread \\(\\tau^2\\).\nBayesian network meta-analysis\nA network meta-analysis relies on a transitivity assumption whereby indirect comparisons can be constructed from direct comparisons. That is, if we have direct comparisons \\(\\delta_{AC}\\) and \\(\\delta_{CB}\\), we can construct an indirect comparison of differences (Lumley 2002). \\[\n\\delta_{AB} = \\delta_{AC} + \\delta_{CB}\n\\]\nUsing this, analogously to \\(BX\\) a coefficients are found for all treatment effects, relative to a control or placebo.\nLumley characterises the structure of the model for three treatments (Lumley 2002). For the \\(k\\)th study’s treatments \\(i\\) and \\(j\\), measured by \\(Y_{ijk}\\), true average effects \\(\\mu\\) and \\(\\eta\\) capturing the heterogeneity of the treatment effect, with \\(\\zeta\\) denoting a change in the effect of treatment \\(i\\) when it is compared to \\(j\\)\\[\nY_{ijk} \\sim \\text{normal}(\\mu_i - \\mu_j + \\eta_{ik} + \\eta_{jk} + \\zeta_{ij}, \\sigma^2_{ijk})\\\\\n\\eta_{ij} \\sim \\text{normal}(0, \\tau^2)\\\\\n\\zeta_{ij} \\sim \\text{normal}(0, \\omega^2)\n\\] This statement lays out the basic structure clearest for me, especially where heterogeneity \\(\\tau^2\\) and incoherence \\(\\omega\\) sit in the structure.\nSince multinma:: provides tools for Bayesian network meta-analysis, our computational model solves for the posterior (McElreath 2016), providing the probability of the parameter of interest, \\(\\theta\\), given the data, \\(x\\), \\[\nP(\\theta|x) = \\frac{P(x|\\theta)P(\\theta)}{P(x)}.\n\\] We get the distributions of the mean value of the parameters, the non-data bits, of the model.\nAnd, using the nomeclature of the documentation in ::nma_thresh, for a fixed-effect model, we have prior \\[\nd \\sim \\text{normal}(d_0, \\Sigma_d)\\\\\n\\] and likelihood \\[\ny\\ |\\ d \\sim \\text{normal}(\\delta, V)\\\\\n\\] with fixed effect \\[\n\\delta = Xd \n\\]\nNow to explore this theory in action with an example network meta-analysis.\nexample dataset: parkinsons_dat\nWe’ll use an example dataset from multinma:: with seven studies’ results for pairwise comparisons of five treatments of interest for our continuous response measure,\n\nthe mean off-time reduction in patients given dopamine agonists as adjunct therapy in Parkinson’s disease from 7 trials comparing four active drugs and placebo (?parkinsons documentation; Phillippo et al. 2020; data source Dias et al. 2011).\n\n\n\n# peek at the data\nparkinsons %>% \n  head(3)\n\n  studyn trtn     y    se   n  diff se_diff\n1      1    1 -1.22 0.504  54    NA   0.504\n2      1    3 -1.53 0.439  95 -0.31   0.668\n3      2    1 -0.70 0.282 172    NA   0.282\n\n# just gon' tweak it a li'l so I can keep track of the variables\nparkinsons_dat <- \n  parkinsons %>% \n  mutate(\n    studyn = str_c(\"study_\", studyn),\n    trtn = str_c(\"trt_\", trtn)\n  )\n\nFrom ?parkinsons documentation, we know we have a study variable studyn, a treatment variable trtn, a measure of effect y, and standard error of the effect, se, sample size n.\nFirst step is to convert the raw data to a network object for contrast analysis.\n\n\n# create network object\nparkinsons_net <-\n  set_agd_contrast(\n    parkinsons_dat,\n    study = studyn,\n    trt = trtn,\n    y = diff,\n    se = se_diff,\n    sample_size = n\n  )\n\n# this is a network object\nparkinsons_net\n\nA network with 7 AgD studies (contrast-based).\n\n---------------------------------------- AgD studies (contrast-based) ---- \n Study   Treatments              \n study_1 2: trt_1 | trt_3        \n study_2 2: trt_1 | trt_2        \n study_3 3: trt_1 | trt_2 | trt_4\n study_4 2: trt_3 | trt_4        \n study_5 2: trt_3 | trt_4        \n study_6 2: trt_4 | trt_5        \n study_7 2: trt_4 | trt_5        \n\n Outcome type: continuous\n--------------------------------------------------------------------------\nTotal number of treatments: 5\nTotal number of studies: 7\nReference treatment is: trt_4\nNetwork is connected\n\nNow we’ve converted our data to a network object, we can inspect the direct and indirect evidence we have. That is which treatments have a pairwise comparison in at least one study.\n\n\nplot(parkinsons_net, weight_edges = FALSE)\n\n\nFigure 1: This network of 7 studies contains direct and indirect (via transitivity) effects, with respect to treatment 4.\n\n\n\nWe can see the model will be creating indirect comparisons between treatments such 2 and 3, using the direct evidence in studies that compare 1 and 3, as well as 1 and 2.\nnetwork meta-analysis\nWe run the following code to fit a random-effects network meta-analysis model.\n\n\nnma_results <-\n  nma(parkinsons_net,\n      trt_effects = \"fixed\",\n      prior_trt = normal(scale = 100))\n\nWe assume a prior distribution on \\(k = 1, \\dots, 5\\) treatment effects \\[\nd_k \\sim \\text{normal}(d_4, 100^2) \n\\] Since we assume a likelihood \\[\ny\\ |\\ d \\sim \\text{normal}(\\delta, V)\\\\\n\\] with fixed effect \\[\n\\delta = Xd\n\\] our model will give us estimates of \\(d\\), \\(mu\\) and \\(V\\).\nWhen we plot the posteriors and with their priors, we see estimates for the comparative difference of treatments from the placebo, treatment 4.\n\n\nplot_prior_posterior(nma_results)\n\n\nWe consider two ways of assessing the results of the model, the standards outlined in Cochrane’s Handbook (Higgins et al. 2019) and threshold analysis (Phillippo 2020).\ncochrane reporting\nNow, we have the a network meta-anlaysis, we turn to 11.6 of Cochrane handbook for how to inspect the results of the model (Higgins et al. 2019).\nevidence base and network structure\nFor networks that do not have an overabundance of nodes, that is treatments, a plot of the network, as shown in Figure 1 is enough. We can also inspect the network with a table.\n\n\nparkinsons_net\n\nA network with 7 AgD studies (contrast-based).\n\n---------------------------------------- AgD studies (contrast-based) ---- \n Study   Treatments              \n study_1 2: trt_1 | trt_3        \n study_2 2: trt_1 | trt_2        \n study_3 3: trt_1 | trt_2 | trt_4\n study_4 2: trt_3 | trt_4        \n study_5 2: trt_3 | trt_4        \n study_6 2: trt_4 | trt_5        \n study_7 2: trt_4 | trt_5        \n\n Outcome type: continuous\n--------------------------------------------------------------------------\nTotal number of treatments: 5\nTotal number of studies: 7\nReference treatment is: trt_4\nNetwork is connected\n\nflow of evidence\nAnother way of summarising the evidence base of a network meta-analysis is a contribution matrix, as described in 11.6.3, however I have as yet to find this.\ncontribution matrix\n\nHaven’t found a tool for this. Might need to code one.\n\nresults\nintervention effects\nThe credible intervals for each of the comparative treatment effects is of most interest, which we can inspect with a forest plot.\n\n\nplot(nma_results)\n\n\nThis would suggest the second treatment has the greatest effect. A table is also a useful summary.\n\n\nnma_results\n\nA fixed effects NMA with a normal likelihood (identity link).\nInference for Stan model: normal.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nd[trt_1]  0.53    0.01 0.47 -0.39  0.22  0.53  0.83  1.46  2279    1\nd[trt_2] -1.28    0.01 0.51 -2.28 -1.62 -1.27 -0.94 -0.29  2396    1\nd[trt_3]  0.05    0.01 0.32 -0.59 -0.17  0.05  0.27  0.68  2688    1\nd[trt_5] -0.30    0.00 0.20 -0.69 -0.43 -0.30 -0.16  0.11  3552    1\nlp__     -3.09    0.03 1.41 -6.61 -3.79 -2.76 -2.04 -1.36  1807    1\n\nSamples were drawn using NUTS(diag_e) at Wed Oct  7 20:56:12 2020.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\nrankograms\nAnother recommendation is to provide the ranking distributions of the different treatments.\n\n\nposterior_rank_probs(nma_results) %>% \n  plot()\n\n\nA nice feature of multinma:: is that it provides cumulative ranking plots, which I think are bit easier to interpret. Here we see the second treatment has the greatest likelihood to be ranked the best treatment.\n\n\nposterior_rank_probs(nma_results, cumulative = TRUE) %>% \n  plot()\n\n\nthreshold analysis\n\nWork in progress.\n\nPhillipo et al. claim threshold analysis provides a more robust method of assessing confidence in recommendations based on network meta-analysis than the widely-used [] GRADE framework (Phillippo et al. 2019). The typical results of a network meta-analysis is a ‘consistent set of treatment estimates so that coherent recommendations may be made’, however there are many reasons we may question the strength of the evidence. In the case of the parkinsons_dat dataset, we have an estimate of the laudifference of a particular treatment on the mean off-time reduction in patients given dopamine agonists as adjunct therapy, from the default treatment.\n\nThreshold analysis quantifies precisely how much the evidence could change (for any reason, such as potential biases or simply sampling variation) before the recommendation changes, and what the revised recommendation would be. If it is judged that the evidence could not plausibly change by more than this amount then the recommendation is considered robust, otherwise the recommendation is sensitive to plausible changes in the evidence (Phillippo et al. 2019).\n\nThreshold analyses are performed at both study level and at contrast level, in either case the result is a set of thresholds that show how much the data point can change before the recommendation changes. We will do a contrast-level threshold analysis on the parkinsons_dat dataset.\n::nma_thresh for random effects model require\nthe posterior means means.dk of the basic treatment parameters \\(d_k\\),\n\n\npost_means <- \n  summary(nma_results, pars=c(\"d\")) %>% \n  as.data.frame() %>%\n  pull(\"mean\")\n\na likelihood lhood covariance matrix,\n\n\n# for lhood\nlikelihood_cov <- \n  diag(parkinsons_dat$se^2)\n\nthe posterior covariance matrix post, posterior covariance mtrix of the vector \\((\\delta^T, \\sigma^T, \\mu^T)^T\\)\n\n\n# for posterior\npost_cov <-\n  summary(nma_results, pars = c(\"d\")) %>% \n  as.data.frame() %>% \n  pull(\"sd\") %>% \n  as.numeric() %>% \n  map_dbl(.f = function(x){x^2}) %>% \n  diag()\n\na design matrix mu.design for additional covariates,\n\n\n# a design matrix for mu.design\nM <- matrix(0, nrow = nrow(parkinsons_dat), ncol = 7)\n\nfor (i in 1:nrow(parkinsons_dat)) {\n  if (parkinsons_dat$trtn[i]) M[i, parkinsons_dat$studyn[i]] <- 1\n}\n\na design matrix delta.design for random effects terms.\n\n\n# construct the design mtrix for eazch contrast\nX <- matrix(0, nrow = 15, ncol = 6)\n\nfor (i in 1:15){\n  X[i, parkinsons_dat$trtn[i]-1] <- 1\n  if (parkinsons_dat$studyn[i] != 4){\n    X[i, parkinsons_dat$studyn[i]-1] <- -1\n  }\n}\n\nWith these inputs, we construct a threshold object.\n\n\n# unfortunately this code throws an error\n# indicating the construction of the posterior covariance matrix is not correct\nthresh <- \n  nma_thresh(\n    mean.dk = post_means,\n    lhood = likelihood_cov,\n    post= post_cov,\n    X = X,\n    nmatype = \"fixed\"\n  )\n\nError in nma_thresh(mean.dk = post_means, lhood = likelihood_cov, post = post_cov, : object 'X' not found\n\nIt will take some more study to figure out how to do a threshold analysis. So, I’ll toolchain walkthrough, an opinionated documentation of a scientific workflow (Gray 2019), through the vignette for threshold analysis, and try to connect it to the structure of the model as mathematical objects. Already I can see the dimensions of the matrices I constructed are not correct.\nthe math of ::nma_thresh\nPrior:\n\\[\nd \\sim N(d_0, \\Sigma_d)\n\\] Likelihood: \\[\ny | d \\sim N(\\delta, V)\n\\] FE model: \\[\n\\delta = Xd + M\\mu\n\\] Which components of these do we need for the threshold analysis?\nthe posterior means means.dk of the basic treatment parameters \\(d_k\\)\n\nSo that would be the \\(d\\) in \\(\\delta = Xd + M\\mu\\)?\n\n\nnmathresh:: example: social anxiety\nIn this section, I toolchain walkthrough through the code provided as an example in Phillipo et al.’s threshold analysis manuscript (Phillippo et al. 2019).\nThe data provided are from a network meta-analysis of social anxiety with 41 treatments in 17 classes over 100 studies.\nthe posterior means means.dk of the basic treatment parameters \\(d_k\\),\nreferences\n\n\nDias, Sofia, Nicky J Welton, Alex J Sutton, and AE Ades. 2011. “NICE Dsu Technical Support Document 2: A Generalised Linear Modelling Framework for Pairwise and Network Meta-Analysis of Randomised Controlled Trials.”\n\n\nGray, Charles T. 2019. “Code::Proof: Prepare for Most Weather Conditions.” Edited by Hien Nguyen. Statistics and Data Science, Communications in Computer and Information Science,, 22–41. https://doi.org/10.1007/978-981-15-1960-4_2.\n\n\nHarrer, Mathias, Prof Dr Pim Cuijpers², Prof Dr Toshi A. Furukawa³, and Assoc Prof Dr David D. Ebert². 2019. Doing Meta-Analysis in R. https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/.\n\n\nHiggins, Julian PT, James Thomas, Jacqueline Chandler, Miranda Cumpston, Tianjing Li, Matthew J Page, and Vivian A Welch. 2019. Cochrane Handbook for Systematic Reviews of Interventions. John Wiley & Sons.\n\n\nLumley, Thomas. 2002. “Network Meta-Analysis for Indirect Treatment Comparisons.” Statistics in Medicine 21 (16): 2313–24. https://doi.org/10.1002/sim.1201.\n\n\nMcElreath, Richard. 2016. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. CRC Press. http://xcelab.net/rm/statistical-rethinking/.\n\n\nPhillippo, David M. 2020. Multinma: Network Meta-Analysis of Individual and Aggregate Data in Stan. Manual. https://doi.org/10.5281/zenodo.3904454.\n\n\nPhillippo, David M., Sofia Dias, A. E. Ades, Mark Belger, Alan Brnabic, Alexander Schacht, Daniel Saure, Zbigniew Kadziola, and Nicky J. Welton. 2020. “Multilevel Network Meta-Regression for Population-Adjusted Treatment Comparisons.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 183 (3): 1189–1210. https://doi.org/10.1111/rssa.12579.\n\n\nPhillippo, David M., Sofia Dias, A. E. Ades, Vanessa Didelez, and Nicky J. Welton. 2018. “Sensitivity of Treatment Recommendations to Bias in Network Meta‐analysis.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 181 (3): 843–67. https://doi.org/10.1111/rssa.12341.\n\n\nPhillippo, David M., Sofia Dias, Nicky J. Welton, Deborah M. Caldwell, Nichole Taske, and A.e. Ades. 2019. “Threshold Analysis as an Alternative to GRADE for Assessing Confidence in Guideline Recommendations Based on Network Meta-Analyses.” Annals of Internal Medicine 170 (8): 538–46. https://doi.org/10.7326/M18-3542.\n\n\n\n\n",
    "preview": "posts/2020-09-06-nma/nma_files/figure-html5/network-1.png",
    "last_modified": "2021-01-19T01:18:21+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-10-24-incorrigible-tidyvert/",
    "title": "incorrigible tidy::vert",
    "description": "a song with %>% references",
    "author": [
      {
        "name": "Charles T. Gray",
        "url": "https://twitter.com/cantabile"
      }
    ],
    "date": "2019-10-24",
    "categories": [
      "music"
    ],
    "contents": "\nTable of Contents\nincorrigible tidy::vert %>%  exegesis()\nthe %>% operator in the R languageWhat is an operator?\nWhat is a composite?\n\nWhy do I love to %>%?\nincorrigible tidy::vert %>%  lyrics()\niphone balanced on music stand quality recording\nincorrigible tidy::vert %>%  exegesis()\n\nThis post will be adapted for inclusion in a joint publication of mathematically-informed artworks engaging with Magritte’s pipe, an icon of surrealism. This forthcoming publication is from a working group of researchers who met at the 2019 Heidelberg Laureate Forum.\n\n\nThank you, Laura Ación, Hao Ye, and James Goldie for checking the maths, and for the reflections on %>%.\n\nIn his 1929 surrealist painting, The Treachery of Images, René Magritte declares Ceci n’est pas une pipe (This is not a pipe). In so doing, he highlights this is but an image, a representation, of a pipe, not truly a pipe itself.\n\nThe Treachery of Images, René Magritte, 1929. Image source: wikipedia.\nI recently wrote a song with lots of references to R and the tidyverse:: metapackage(Wickham 2017).\n\nI didn't mean to %>% your bubble\nI'm just an incorrigible tidy::vert\nIn this post, I’ll unpack the pipe operator, %>%, that features throughout the lyrics.\nthe %>% operator in the R language\n\n\n\nFigure 1: Image source: magritrr:: GitHub repository.\n\n\n\nThe %>% pipe operator is the first major concept introduced in the programming section, following exploratory data analysis and data wrangling, of Wickham’s R for Data Science (Grolemund and Wickham 2017).\nWith Stefan Milton Bache’s magrittr::(Bache and Wickham 2014) package,\n\n\n# f(x) is equivalent to x %>% f() with magrittr::\n\nlibrary(magrittr)\n\n# f(x)\nround(3.1)\n\n[1] 3\n\n# is equivalent to x %>% f()\n\n3.1 %>% round()\n\n[1] 3\n\nWhat is an operator?\nWe often forget that operators are, themselves, functions.\nBrian A. Davey’s MAT4GA General Algebra coursebook (what I have on hand) provides this definition.\nFor \\(n \\in \\mathbb N_0 := \\mathbb N \\cup \\{ 0\\}\\), a map \\(f : A^n \\to A\\) is called an n-ary operation on A.\nFor example, \\(+\\) is a function that takes two arguments, numbers, and returns a single number. Algebraically, 3 + 2 = 5 is shorthand for +(3, 2) = 5.\nFor those with formal mathematical training, multiple uses of the %>% operator in a single line of code can be thought of in terms of a coding instantiation of a composite of functions.\nWhat is a composite?\nLet \\(f\\) and \\(g\\) be real functions.\nThe composite of \\(f\\) with \\(g\\) is the real function \\(g \\circ f\\) given by the formula \\((g \\circ f)(x) := g(f(x))\\).\nFor reasons that only made sense to me once I reached graduate-level mathematics, we read a composite of functions from right to left.\nAnd just to break our brains a little, algebraically, the composite operator is a function, so we have \\(g \\circ f = \\circ (f, g)\\)!\nThe pipe, %>%, operator is the R-language equivalent to the composite \\(\\circ\\) operator on real functions.\nWhy do I love to %>%?\nHere is an example with three functions: \\((h \\circ g \\circ f)(x) := h(g(f(x))).\\)\n\n\nset.seed(39)\n\n# get a random sample size between 20 & 100\nsample(seq(20, 100), 1) %>% # this f(x) goes into\n  # generate sample from normal distribution with \n  # mean 50 & sd 0.5\nrnorm(., 50, 0.5) %>% # g, so, now g(f(x), which goes into\n  # calculate mean of that sample\n  mean() # h, so h(g(f(x)))\n\n[1] 49.94228\n\nTo see how this is the \\((h \\circ g \\circ f)(x)\\) instantiation, reading from right to left, we take a look at the \\(h(g(f(x)))\\) instantiation of the same code.\n\n\n# this line of code is equivalent to above\n# h(g(f(x))) is less text\n# but the algorithm is harder to ascertain \nmean(rnorm(sample(seq(20, 100), 1), 50, 0.5))\n\n[1] 49.98828\n\nThe reader is invited to consider if they agree with the author that it is harder to read the symbols so close together, in this \\(h(g(f(x)))\\) instantiation of the code. Also, arguably more importantly, one does not have the ability to comment each component of the algorithm.\nThere is a downside to the %>%, however. The longer a composite becomes, the more difficult it is to identify errors.\n\n\n \nOn the the train Leipzig with snow falling\nAnd my %>%s are getting too long\n\n\nincorrigible tidy::vert %>%  lyrics()\n\n\n\n\nCaught the train to Leipzig, snow is falling\nBut I am not nearly done\nRube Goldberging this algorithm\nBut the sampling is off.\n\nI didn't mean to %>% your bubble\nI'm just an incorrible tidy::vert\n\nAnd I'm here to tell ya\nThere's some rhyme and reason\nBut there's a whole lot that can get fucked up\n\nAnd I'm here to tell ya\nThere's scarce rhyme and reason\nSo let's brace for the shitstorm\n\nDon't think I'll unravel\nThe mysteries of the beta distribution\nOn the the train Leipzig with snow falling\nAnd my %>%s are getting too long\n\nBut I didn't mean to %>% your bubble\nI'm just an incorrible tidy::vert\n\nAnd I'm here to tell ya\nThere's some rhyme and reason\nBut there's a whole lot that can get fucked up\n\nAnd I'm here to tell ya\nThere's scarce rhyme and reason\nSo let's brace for the CRANstorm\n\nAnd I didn't mean to %>% your bubble\nLet your flame war flame itself out\nAnd I didn't mean to %>% your bubble\nExcuse Me, Do You Have a Moment \n to Talk About Version Control?\n\nAnd I didn't mean to %>% your bubble\nI'm just an incorrible tidy::vert\nAnd I didn't mean to %>% your bubble\nI'm just an incorrible tidy::vert\n\n\niphone balanced on music stand quality recording\nDon’t say I didn’t warn you about the sound quality.\n\n\n\n\nBache, Stefan Milton, and Hadley Wickham. 2014. “Magrittr: A Forward-Pipe Operator for R.”\n\n\nGrolemund, Garrett, and Hadley Wickham. 2017. R for Data Science.\n\n\nWickham, Hadley. 2017. “Tidyverse: Easily Install and Load the ’Tidyverse’.”\n\n\n\n\n",
    "preview": "posts/2019-10-24-incorrigible-tidyvert/pipe-logo.png",
    "last_modified": "2021-01-19T01:18:21+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-03-28-quack-quack-said-the-duck/",
    "title": "quack! quack! said the duck",
    "description": "making sense of methods in R",
    "author": [
      {
        "name": "cantabile",
        "url": {}
      }
    ],
    "date": "2019-03-28",
    "categories": [
      "dev"
    ],
    "contents": "\nTable of Contents\nafter a cursory look at UseMethod documentation\nmake a duck quack, with a little help from Josiah\nstuff I still don’t get\nmeritgeta duck for maelle\n\nSome time back, @malco_barrett and I discovered we both had tidyverse::-integrated functions in development for meta-analysis models, specifically models produced by metafor::rma.\nWe started to discuss how to bring together the work we’d done, and then, speaking for myself, I got busy and overwhelmed last year with the general craziness that doing a doctorate is, and this slipped way back on the backburner.\nLast week, however, one of the creators of the broom:: package got in touch with Malcolm about porting the code. And, with the Evidence Synthesis Hackathon imminent, it seems as if the time is nigh. If I’m going to be able to collaborate on this, that means finally getting around to learning about methods in R.\n\n\nCracks metaphoric R knuckles.So, S3. What's that all about?\n\n— Charles T. Gray (@cantabile) March 27, 2019\n\nI think I might find this pedagogically interesting in future, to reflect on where I started today, and where I ended up.\n\n\nJourney begins here: my current impression is that, digging back 8 years ago to my one compsci subject, first year java, S3 is a method for an object.\n\n— Charles T. Gray (@cantabile) March 27, 2019\n\nafter a cursory look at UseMethod documentation\nRecently someone, I think it was @kiersi, was asking on twitter how people learn. Today I learnt that I like to play first, ask questions of the documentation after.\nMike Penguin said something about UseMethod on twitter, so I thought I’d start there.\n\n\nOnce I realized that myfun <- function(x, …) UseMethod(“myfun”) is the boilerplate needed I was away. It does magic so that the methods you write i.e. myfun.myclass work like myfun(x) where class(x) includes“my class”\n\n— Michael Sumner (@mdsumner) March 27, 2019\n\n\n\n# i guess i need an object for my function to act on\nfluffyduck <- \"i am a duck\"\n\n# i wonder if you can define your own class?\nclass(fluffyduck) <- \"duck\"\nclass(fluffyduck)\n\n[1] \"duck\"\n\n# the UseMethod documentation said the method needs to act on an object\nis.object(fluffyduck)\n\n[1] TRUE\n\n# make a function \nfirstquack <- function() {cat(\"quack\")}\n\n# see if it works\nfirstquack()\n\nquack\n\n\n\n# but this throws an error\nUseMethod(\"firstquack\", duck)\n\nError in eval(expr, envir, enclos): object 'duck' not found\n\n# okay, time to look to the tubes for help\n\nmake a duck quack, with a little help from Josiah\n\n\nThis! I wrote a post about it and used my PR to janitor as an example. https://t.co/Vn7H3BAQjm\n\n— Josiah Parry (@JosiahParry) March 28, 2019\n\nJosiah’s post was most helpful, indeed. These are my notes.\n\n\n# start by creating a \"generic\" function\nquack <- function(says_the_duck, greeting = \"quack!\") {\n  UseMethod(\"quack\")\n}\n\nThe generic function seems like an instantiation step. I read this as, I will create a function called this that I can control how it behaves for different classes.\n\n\nquack() # okay, same error as josiah's, so far so good\n\nError in UseMethod(\"quack\"): no applicable method for 'quack' applied to an object of class \"NULL\"\n\nSidenote, set chunk option error=TRUE to display code and output for an error.\nMakes sense that this throws an error, we haven’t told it how to behave for anything yet.\n\n\n# set up a default method\nquack.default <-\n  function(says_the_duck = \"quack!\",           greeting = \"quack! \") {\n    print(paste0(greeting, says_the_duck))\n    cat(\"said the duck\")\n  }\n\n# check this default works for anything\nquack(NULL) # yay! it quacked on NULL\n\n[1] \"quack! \"\nsaid the duck\n\nquack() # and no argument?\n\n[1] \"quack! quack!\"\nsaid the duck\n\nquack(\"i'm a duck\") # a string?\n\n[1] \"quack! i'm a duck\"\nsaid the duck\n\n# now to make a fluffyduck-class object\nfluffyduck <- \"i am fluffy\" # create an object\nclass(fluffyduck) <- \"fluffyduck\" # set class of object\n\n# trying this before referring back to the post\nquack.fluffyduck <- function(says_the_duck, greeting = \"quack! \") {\n  print(paste0(greeting, says_the_duck))\n  cat(\"said the fluffy duck\\n\")\n}\n\n# does the duck quack?\nquack(fluffyduck)\n\n[1] \"quack! i am fluffy\"\nsaid the fluffy duck\n\n# try another object\nanother_fluffy_duck <- \"i am the fluffiest!\"\n\n# test default\nquack(another_fluffy_duck)\n\n[1] \"quack! i am the fluffiest!\"\nsaid the duck\n\n# but this duck is also fluffy!\nclass(another_fluffy_duck) <- \"fluffyduck\"\nquack(another_fluffy_duck)\n\n[1] \"quack! i am the fluffiest!\"\nsaid the fluffy duck\n\n# now to test changing my default greeting, to understand how to parse other arguments\nquack(another_fluffy_duck, greeting = \"a most fluffy day to you, \")\n\n[1] \"a most fluffy day to you, i am the fluffiest!\"\nsaid the fluffy duck\n\nstuff I still don’t get\nI don’t understand what all the words mean in these definitions of S3 and S4.\n\n4 works similarly to S3, but is more formal. There are two major differences to S3. S4 has formal class definitions, which describe the representation and inheritance for each class, and has special helper functions for defining generics and methods. S4 also has multiple dispatch, which means that generic functions can pick methods based on the class of any number of arguments, not just one. - Advanced R\n\nDoes multiple dispatch mean that the method can be conditional on the class of more than one argument?\nThis was fun; despite me finishing with more questions than I began with.\nmeritget\nMuch obliged to @jacinta for suggesting I update; the internet’s highest honour, gif-phrased praise.\n\n\nFluffy duck! :) pic.twitter.com/9HUPVaXAL4\n\n— Dr Jenny Richmond (@JenRichmondPhD) March 28, 2019\n\na duck for maelle\n\n\n# a duck for maelle!\nmaelles_duck <- \"je suis une cane\"\nclass(maelles_duck) <- \"frenchduck\"\n\n# french ducks speak in french\nquack.frenchduck <- function(says_the_duck, greeting = \"coin! \") {\n  print(paste0(greeting, says_the_duck))\n  cat(\"dit la cane\\n\")\n}\n\n# coin?\nquack(maelles_duck)\n\n[1] \"coin! je suis une cane\"\ndit la cane\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-19T01:18:21+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-02-19-wrangling-errors/",
    "title": "wrangling errors",
    "description": "filtering a list by type",
    "author": [
      {
        "name": "Charles T. Gray",
        "url": "https://twitter.com/cantabile"
      }
    ],
    "date": "2019-02-19",
    "categories": [
      "wrangling",
      "error handling",
      "debugging"
    ],
    "contents": "\n\n\n# packages used in this post\nlibrary(tidyverse)\n\nI want to wrangle some errors out of the results of a function.\nAt the moment, my function returns NULL if a warning or an error is thrown, which gets me the results that ran, but I’d like to have more information about the trials that didn’t run.\nI think I can get my function to return a dataframe of results if the function works as intended, and a character string detailing the error or warning if the function fails. This will give me a list of dataframes intermixed with character strings.\nSo, my question of the day is how to filter a list by type?\nFirst, I’ll create a dummy list1.\n\n\n# create a list of dataframes and character strings\nplaylist <- list(\n  \"Beanie 'Legs' McGraw\",\n  \"Peug the Door-opening Cat\",\n  iris %>% select(Sepal.Length, Species) %>%  filter(Species == \"setosa\") %>% head(),\n  iris %>% select(Sepal.Length, Species) %>% filter(Species == \"versicolor\") %>% head(),\n  iris %>% select(Sepal.Length, Species) %>% filter(Species == \"virginica\") %>% head(),\n  \"Lord Euclid of the Fluffy Butt\"\n)\n\nplaylist %>% str()\n\nList of 6\n $ : chr \"Beanie 'Legs' McGraw\"\n $ : chr \"Peug the Door-opening Cat\"\n $ :'data.frame':   6 obs. of  2 variables:\n  ..$ Sepal.Length: num [1:6] 5.1 4.9 4.7 4.6 5 5.4\n  ..$ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1\n $ :'data.frame':   6 obs. of  2 variables:\n  ..$ Sepal.Length: num [1:6] 7 6.4 6.9 5.5 6.5 5.7\n  ..$ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 2 2 2 2 2 2\n $ :'data.frame':   6 obs. of  2 variables:\n  ..$ Sepal.Length: num [1:6] 6.3 5.8 7.1 6.3 6.5 7.6\n  ..$ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 3 3 3 3 3 3\n $ : chr \"Lord Euclid of the Fluffy Butt\"\n\nSo, how do I get the dataframes out into one list and the errors into another? Once separated, I’ll be able to bind_rows into two dataframes, results and errors.\nOff the top of my head, I can see how to do this with map, at least for the dataframes.\n\n\n# extract elements that are dataframes\nplaylist %>% \n  map(.f = function(x){\n    if (is.data.frame(x)) return(x)\n  }) %>% bind_rows()\n\n   Sepal.Length    Species\n1           5.1     setosa\n2           4.9     setosa\n3           4.7     setosa\n4           4.6     setosa\n5           5.0     setosa\n6           5.4     setosa\n7           7.0 versicolor\n8           6.4 versicolor\n9           6.9 versicolor\n10          5.5 versicolor\n11          6.5 versicolor\n12          5.7 versicolor\n13          6.3  virginica\n14          5.8  virginica\n15          7.1  virginica\n16          6.3  virginica\n17          6.5  virginica\n18          7.6  virginica\n\nSo that seems to work and didn’t take too much code. Good enough. (I checked, and the function defaults to returning NULL if the condition is not met.)\nBut applying the same logic to filtering the character strings out\n\n\n# extract elements that are character strings\nplaylist %>% \n  map(.f = function(x){\n    if (is.character(x)) return(x) \n  }) %>% as.character()\n\n[1] \"Beanie 'Legs' McGraw\"           \"Peug the Door-opening Cat\"     \n[3] \"NULL\"                           \"NULL\"                          \n[5] \"NULL\"                           \"Lord Euclid of the Fluffy Butt\"\n\nHow to get rid of the NULL elements? I suppose I could do some base, but there’s probably a nifty tidy way around this.\nI suspect, however, that there is a better way to filter lists with purrr:: combined with dplyr::. Possibly scary lambda functions? I need to level up my purrr::.\ntweeps to the rescue\nThanks to James, Ken, and Francois, I found the purrr::keep function. Just what I wanted for Christmas! Cheers.\n\n\nHey, purrr:: tweeps. Anyone know offhand about filtering a list by type? Here's a quick post outlining what I'd like to do, and my hack. @samclifford @rensa_co, perchance? https://t.co/BxpXXfHUBc\n\n— Charles T. Gray (@cantabile) February 19, 2019\n\n\n\nplaylist %>% keep(is.data.frame) %>% bind_rows()\n\n   Sepal.Length    Species\n1           5.1     setosa\n2           4.9     setosa\n3           4.7     setosa\n4           4.6     setosa\n5           5.0     setosa\n6           5.4     setosa\n7           7.0 versicolor\n8           6.4 versicolor\n9           6.9 versicolor\n10          5.5 versicolor\n11          6.5 versicolor\n12          5.7 versicolor\n13          6.3  virginica\n14          5.8  virginica\n15          7.1  virginica\n16          6.3  virginica\n17          6.5  virginica\n18          7.6  virginica\n\nplaylist %>% keep(is.character) %>% as.character()\n\n[1] \"Beanie 'Legs' McGraw\"           \"Peug the Door-opening Cat\"     \n[3] \"Lord Euclid of the Fluffy Butt\"\n\nw00t\nI know, I know, I should find a more interesting dataset than iris.↩\n",
    "preview": {},
    "last_modified": "2021-01-19T01:18:21+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-02-03-fuzz-testing/",
    "title": "fuzz testing",
    "description": "a neat testing concept",
    "author": [
      {
        "name": "Charles T. Gray",
        "url": "https://twitter.com/cantabile"
      }
    ],
    "date": "2019-02-03",
    "categories": [
      "testing"
    ],
    "contents": "\nCame across this neat testing concept recently, fuzz testing.\nI was working through some primers on testing, I got to thinking about equivalence class partitioning in algebra (I had a proof on equivalence class partitions in my thesis) as a means of exploring the question, for what values of <type> does my function <fn> run?\nAnyway, I got to thinking it may well not entirely be clear to another user what assumptions were made. And these things can matter. For me, it’s often the difference between whether my code runs or not.\nrandomising testing inputs\nI’d been experimenting with randomly sampling testing parameters, trying to cover my bases, as it were. But I didn’t feel clear on how to go about it until I was working through these primers recently.\nRealising the partitions will make explicit my assumptions is really handy. Especially for future Charles. Past Charles has a way of assuming that future Charles will intuit what was the obvious thing to check.\ntesting numerics\nTake for example a numeric argument <arg> for a function <fn>. I’ve been tripped up many a time by negative numbers going into a log. So I might make sure I test a positive number, a negative number, and 0.\ntest\nequivalence class\n<arg> =\npositive\n\\([-\\infty, 0)\\)\nrunif(1, -100, 0)\n0\n\\([0,0]\\)\n0\nnegative\n\\((0, \\infty]\\)\nrunif(1, 0, 100)\ncan we use \\(\\infty\\) as an argument in runif?\nNo.\n\n\n> runif(1)\n[1] 0.567648\n> runif(1, -inf)\nError in runif(1, -inf) : object 'inf' not found\n> inf\nError: object 'inf' not found\n> Inf\n[1] Inf\n> runif(1, -Inf)\n[1] NaN\nWarning message:\nIn runif(1, -Inf) : NAs produced\n\nSo, we need to choose an arbitrarily large number. Say, 100.\nIndeed, what about small numbers, particularly those between 0 and 1?\nAnd, what about \\(\\pm 1\\)? It is always a special case because of cancelling effects. Could cause me trouble.\nupdated partitions\ntest\nequivalence class\n<arg> =\nnegative\n\\([-\\infty, 0)\\)\nrunif(1, -100, -1)\n-1\n\\([-1,-1]\\)\n-1\nsmall negative\n\\((-1, 0)\\)\nrunif(1, -1, 0)\n0\n\\([0,0]\\)\n0\nsmall positive\n\\((0, 1)\\)\nrunif(1, 0, 1)\n1\n\\([1,1]\\)\n1\npositive\n\\((1, \\infty]\\)\nrunif(1, 1, 100)\nmeh, stop somewhere\nBut what if 100 is not large enough? What if <fn>(<arg>) is fine if <arg> < 100, but fails if <arg> = 150? (The things that keep us up at night.) Meh, at some point you need to call it. For these analyses, I don’t think I’ll worry about that for now. I think going as big as 100 should cover my bases.\nSo what I like about this is my tests make it clear what contingences I’ve prepared for, and what I haven’t.\nothers types are easier\nFinally, the good news is that other types are easier to partition. Logicals have only TRUE and FALSE inputs.\nfuzz testing\nI pestered the author of the primers I was working through, Greg Wilson, with these thoughts, and he put me onto this concept of fuzz testing.\nupdate: reproducibility\nSomething I forgot to mention in this post, that came up in discussion when I posted this to twitter was reproducibility. How to ensure another can reproduce my results, and, in particular, my errors. Especially when I need help. And I always need help.\n\n\nIf you're using random numbers though, how do you know what value caused a failure? So you set the seed in each test file, maybe by date?\n\n— Heather Turner (@HeathrTurnr) February 4, 2019\n\n\n\n# include this at the top of your testing script for reproducibility\nset.seed(<pick a number>)\n\n\na question\nHeather Turner raised a question, that I myself also wondered about.\n\n\nTakes away the randomness though, which might be useful here. Something related to current date/time may be better as long as the expected result is still predictable.\n\n— Heather Turner (@HeathrTurnr) February 4, 2019\n\nMy current solution is to remove set.seed at the moment, while I’m playing around with the functions, but to use set.seed when I’m collaborating. And intend to set it for publication, but that’s a fair more testing aways as of yet.\nI’m curious whether people, especially with development training, have insight?\n\n\n",
    "preview": {},
    "last_modified": "2021-01-19T01:18:21+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-05-its-not-not-the-math-its-the-code/",
    "title": "it's not the maths, it's the code",
    "description": "how testing has changed my workflow",
    "author": [
      {
        "name": "Charles T. Gray",
        "url": {}
      }
    ],
    "date": "2019-01-14",
    "categories": [
      "error handling",
      "testing"
    ],
    "contents": "\nTable of Contents\ndebugging, bane of my existence\nthe dark days\nand now, with testing, less murky\nauto_test workflow junkieconsole work is testing\nanalyses as functions\nhonesty is hard\nworkflow changer\n\ndebugging, bane of my existence\nAs a student of mathematical science, I’ve worked on projects in various disciplines, from ecology to medicine. I’ve clocked up about five years of R use. Although the projects have been varied, one constant has been that debugging has disproportionately dominated my time.\nI find this frustrating, as I would rather work on mathematics. I’m not a computer scientist, and I don’t feel like debugging plays to my strengths. Over the last three years, I’ve become an evangelistic devotee of the tidyverse::(???) because it saves me so much time in terms of data wrangling. Surely there are similar tools for debugging?\nBut of course there are. I’m now convinced that the time I spend learning these tools is time gained in efficiency. Indeed, apparently I am following a well-worn path.\n\nI started using automated tests because I discovered I was spending too much time re-fixing bugs that I’d already fixed before.(Hadley Wickham(???))\n\nI was looking into debugging and I came across this.\n\nIf your existing test coverage is low, take the opportunity to add some nearby tests to ensure that existing good behaviour is preserved. This reduces the chances of creating a new bug.(Again, Hadley Wickham(???). Should put out a line of fortune cookies.)\n\nI can see the benefit of tests1 for developing the work. I often reuse functions and rework analyses, so having an automated system of checking that functions do what you think they’ll do makes sense. Seems like a two birds one stone thing, I can increase testing coverage of my package while debugging the problem I have. It’s also the answer to, where to start with testing?\nI am trying to finish an analysis at the moment which has some estimators I’ve coded. I’m trying to write simulations using these estimators. As usual, I run into problems with my simulations, but I struggle to understand where the problems are. Is it in the structure of the simulation, or in the mathematics, or in the mathematical assumptions?\nTesting via testthat:: enables me to keep track of what I’ve checked, and rechecks it automatically for me. I’ve even become a tentative testthat::auto_test (sotto robot voce) convert.\n\nBy Source, Fair use, Link\n\nHere’s my big revelation so far. Sometimes it’s the code, not the mathematics. Testing has shown me I should spend as much time worrying about whether my code works structurally as I should about the mathematics. For example, does this function always produce a data.frame?\nOftimes it’s my code that’s the problem, not the maths. And sometimes, after all, it’s the maths2. Anyway, testing helps with both, at least to some extent.\nI was really surprised how useful it was to simply test out that the function returns the expected type of result. I build these algorithms that feel like Rube Goldberg machines by the time I’ve done with them, and I’m usually not convinced I’ve not put in several steps for which there was a better shortcut. The things that keep us up at night3.\nI think working testing into my workflow is a lot like learning to %>%. It’s arguably harder to learn when a workflow has already been locked in place. I think this is a struggle for R users of all sorts of levels. However, I think testing is a worthy skill in terms of time benefits, just as learning to %>% saved me so much in debugging. And then some. The %>% changed my workflow and the structure of my code. I’m surprised to find I can use testing as I build the analysis itself. I’ve started using testing all the time.\nBut I don’t think the benefits of testing are immediately apparent for mathematical scientists. They weren’t for me.\nSimply testing each argument in turn is a great place to start, I discovered. Then you can consider all possible inputs for that. And that felt overwhelming.\nthe dark days\nSo, how do I check functions usually? Well, I was writing functions into .Rmd files. Then I’d have an eval=FALSE chunk in there, for these testing parameters.\n\n\n# test parameters\nsample_size = 3\npar_1 = 3\npar_2 = NULL\nrs_fn = rexp\n\nThese would be my testing parameters. I’d set them up in a chunk, so that I could play around with different values.\nThe problem with this is it’s not reproducible. Most importantly, by me. I find it really hard to remember from day to day what I’ve tested ad hoc, and find myself repeating checks compulsively.\nand now, with testing, less murky\nLet’s say these parameters went into a function which outputs a numeric results. Even having an expectation like\n\n\nexpect_is(<my_function>(3, 3, NULL, rexp), \"numeric\")\n\nis really useful. I can also check for multiple inputs at once.\nEspecially as I often have functions that rely on other functions. So changing one can break others really easily if I’m not careful.\nauto_test workflow junkie\ntestthat:: has this lovely function auto_test. From the documentation,\n\nThe idea behind ‘auto_test()’ is that you just leave it running while you develop your code. Everytime you save a file it will be automatically tested and you can easily see if your changes have caused any test failures(???).\n\nA key feature of testthat::auto_test is that the console becomes inaccessible, and all tests run automatically. It was a bit terrifying at first, but I started to write all my code with this function running.\nFor the last week or so, I’ve been experimenting with developing my analysis this way. Here’s what’s changed so far:\nconsole work is testing\nI discovered that much of what I do at the console could be written as an expectation. For example, when I run this function, does it return a dataframe? is the kind of question I frequently ask at the console of a function.\nanalyses as functions\nI began to write all my code as functions. Not just the bits I want to repeat. I started writing my analyses as functions that have tests associated.\nPreviously, I’d followed the rule that if I would need to copy and paste it more than twice, I’d make it a function.\nI’d work from a .Rmd file and run chunks, try bits out.\nThe problem with this was I’d feel like I was starting at the start each day, trying to find the issues in the code.\nhonesty is hard\nI found I had to not infrequently fight the compulsion to stop auto_test, and thus free up the console. But I made it a rule that I had to switch back if what I found myself doing could be phrased as an expectation. I was forced to switch back after a couple of minutes each time.\nHere’s the thing. Much of what I do at the console is answer these questions,\ndoes it run? (or, in the case of data, is it a non-empty thing of <type>?)\nif I put this is in, does it run?\nis this in it its output?\nis this a number?\na positive number?\na big number? (say, bigger than 1?)\nAnd all of these can be written as expectations.\nFor example, here’s a commented example from a few days ago. In the past I would have checked these things as I wrote the code, but I’ve have no record of checking those things.\n\n\ntest_that(\"simulation parameter set up\", {\n  expect_is(meta_df(), \"data.frame\") # does fn output a dataframe?\n  expect_is(meta_df() %>% pluck(\"rdist\"), \"character\") # is this variable a string?\n  expect_is(meta_df() %>% pluck(\"n\"), \"list\") # is this variable a list?\n   # is <column name> a column name in the output of this?\n  expect_true(\"median_ratio\" %in% colnames(meta_df()))\n  expect_true(\"true_median\" %in% colnames(meta_df()))\n  \n  # does changing the value of the argument prop affect the running of the function?\n  expect_is(meta_df(prop = 0.4), \"data.frame\")\n  expect_is(meta_df(prop = prop), \"data.frame\")\n})\n\nworkflow changer\nBy writing tests, I both solve my problem, and keep of log of what I’ve tried and what works. I find this is especially useful for sanity checks. And that I need a lot more sanity checks (i.e, does this function throw an error?) than I thought I did.\nI’m genuinely startled by how much this has influenced my workflow. I thought I would employ these tools specifically for debugging, troubleshooting a specific error, not for development.\n\n\nWhen my husband asks me what I’ve been doing at work, I can’t help but answer by dancing the robot.\nR Packages has a great section on testing, which I myself must study more.↩\nIs it peculiarly Australian to say maths?↩\nIncidentally, I realised yesterday that because I’ve been stop-starting this analysis for two years, I wasn’t coding it tidy, because that’s the way I’d always done it. A supervisor asked me about it a few weeks ago, and now I’ve been turning it over in my head. For no better reason. It’s interesting to see what I’m blind to as I learn. Sometimes it would take years of practicing every day for what a piano teacher had said to me to sink in. This was especially true for the technique of slurring.↩\n",
    "preview": {},
    "last_modified": "2021-01-19T01:18:21+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-13-curiousity-science-letter/",
    "title": "curiousity science letter",
    "description": "Curiosity Science asked me to write a letter",
    "author": [
      {
        "name": "Charles T. Gray",
        "url": {}
      }
    ],
    "date": "2019-01-13",
    "categories": [
      "community"
    ],
    "contents": "\nDear Curiosity Science friends,\nThanks for asking me to write a letter about why I love my job in mathematical science. I went to the Curiousity Science website to be inspired by the awesome letters posted there and found myself thinking, gosh, I certainly feel like no one’s role model.\nI’m currently doing a doctorate in statistical metaresearch, and role model status notwithstanding, I think I have the best job in the world.\nI spend the vast bulk of my time learning statistical computing, thinking about how mathematical algorithms are interpreted, and how we really can make science easier while making it better. And indeed, I feel we must.\n\n\n#credrev18 @cantabile bridging the tool chain gap and reminding us on important truths. And spiderman. pic.twitter.com/lmNvQFAwrf\n\n— Fiona Fidler (@fidlerfm) November 14, 2018\n\nWe are living through a data revolution; we can draw powerful conclusons we couldn’t before. Making sure we get these methods of drawing conclusions right seems useful; I feel like I’m learning to do something that’s really useful in science, and that’s exciting.\nIn the last year, I have been delighted to travel the world consulting with researchers with this shared interest in how we can science as best we can, and how we can help others do the same.\nIn addition to feeling genuinely connected with why what I do is important, I find the work itself really relaxing and fun.\nI spend my time programming or doing mathematics, but I’m always working toward making a picture with the data, like this one from last week.\n\nMaking pictures with my data is my favourite. When the picture aligns with what I think the mathematics is telling me it should look like, I feel like I’m clocking a level on a video game.\n\n\n\nvia GIPHY\n\nNow, you might be thinking, so why would you say you’re not a role model? This all sounds pretty impressive.\nWell, here’s the thing. I did everything wrong.\nI dropped out of high school three times, yup, count ’em.\nI also first became a musician. I feel like I make use of the skills I developed as a musician in my practice of mathematical science every day. Musicians work alone, they think creatively, they solve problems, they have many skills, such as reading music, theory, and so forth. In many ways music is a lot like mathematical science.\nI didn’t start taking an interest in data and mathematics until I was thirty. I was tired of odd jobs and freelancing, and life wasn’t getting in the way so much anymore. So I thought I’d skill up in something that we were meant to have a shortage of. I never dreamt I would do so much with this.\n\n\nErin asked what advice I have to give.\nFirst, if anyone is a role model, it’s Erin. She is someone who looks at a crappy situation, like how girls are often discouraged from science, and say, what can I do about it? These people are my role models.\nAnd for some reason a whole lot of people have felt compelled to tell me that I’m not smart enough to this over the years. Here’s what I think, the thing that matters is how much you enjoy learning about something; not what you already know. If you enjoy learning mathematics or programming, there is a whole world of fun problems to solve out there with those skills.\nBest, Charles\n\n\n",
    "preview": {},
    "last_modified": "2021-01-19T01:18:21+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-27-scientific-lit-review/",
    "title": "writing a scientific literature review",
    "description": "What makes a _good enough_ literature review?",
    "author": [
      {
        "name": "Charles T. Gray",
        "url": {}
      }
    ],
    "date": "2018-11-27",
    "categories": [],
    "contents": "\nAt the outset of a literature review, I’ve learnt the hard way to anticipate that you will not be able to cover the entirety of the literature. Perhaps a very pertinant paper did not come up in your journal search. Perhaps you used the wrong search tool, perhaps the keywords of interest are different in different disciplines. At any rate, a best practice literature review could well fall into the category of how long is a piece of string1?\nThis naturally leads to the question, what is a analogously good enough(???) literature review? My instinctive response, is a literature review that answers the question posed of it, that is, the follows the line of inquiry. But I have a feeling that reading and defining the question are a symbiotic process whereby I refine the question as I read peoples’ thoughts.\nFor example, I began with\n\nread literature on open science and reproducibiility, what are key concepts?\n\nBut the inquiry began to be refined almost immediately as I discovered literature focussing specifically on building a reproducible analysis (???).\n\nwhat work has been done in statistical open science and reproducibility, and what work needs to be done?\n\nFurthemore, statistical software development is intrinsically linked to reproducibility for statistics, so it is impossible to consider the above question without investigating tools developed specifically for reproducible data analysis such as workflowr::(???) and rrtools::(???).\nToday I find myself asking,\n\nwhat does this paper confirm, nuance, or challenge about open and reproducible data analysis?\n\nPerhaps there is more to this, but after taking a look at some lit review guides(???; ???), and establishing there was little in guidance for the type of literature review I’m writing on google scholar. A cursory look at the generalist guides gives me the impression that this something to use common sense for. A clear message, narrative etc. Find both the question and the present the answer, comparing and contrasing nuances of opinion and so forth.\nFinishing this post off, I began to wonder if it was worth writing at all. But then I realised that I feel reassured I’m on the right track now, which absolutely makes it a worthwhile effort. The primary purpose of this blog is to write things down so I feel less anxious about my work. And now I do. :)\nIncidentally, how long is a piece of string was said to me in one of the most infuriating moments of my brief and frustring career in bioinformatics.↩\n",
    "preview": {},
    "last_modified": "2021-01-19T01:18:21+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-12-testing-as-debugging/",
    "title": "testing as debugging",
    "description": "Thoughts on writing my first tests.",
    "author": [
      {
        "name": "Charles T. Gray",
        "url": "https://twitter.com/cantabile"
      }
    ],
    "date": "2018-07-01",
    "categories": [
      "testing",
      "debugging"
    ],
    "contents": "\nI’m writing my first tests at the moment.\nI’ve been rather enamoured with the idea of testing for about six months now, but haven’t had much of a chance to work on research.\nSo, I’m finally now sitting down to begin my first package in earnest, and thus writing my first tests. Seeing as I had a request, I thought I’d write my thoughts at this fairly early point. I must confess, however, that this post wandered around a bit and I’m not sure how useful John would find it, alas; it’s not really an explanatory post, more of an exploration. For actually useful information about getting started with testing, I recommend R packages.\n\n\n\nAny plans to write about this? Now and again I look at test that but have no idea where to start.\n\n— John MacKintosh (@_johnmackintosh) June 30, 2018\n\n\nI recently saw Kara Woo speak at R-Ladies Seattle, and she made a compelling case for debugging with rigour. Admittedly I’ve only spent one evening reading about that before getting distracted by the next shiny thing (i.e., hyperventilating into a figurative paper bag as useR! 2018 approaches.). However, I came across this wonderful gem that has me alight with newfound testing enthusiasm:\n\n“If you’re using automated testing, this is also a good time to create an automated test case. If your existing test coverage is low, take the opportunity to add some nearby tests to ensure that existing good behaviour is preserved. This reduces the chances of creating a new bug.” (Advanced R)\n\nNow, don’t get me wrong, Kara’s inspired me to eventually get across the whole debugging gambit. But it is high conference season, and I’ve got some simulation problems to work out, so I can’t justify spending an overabundance of time on git.\nAdopting this idea into my workflow is enough for now; with this one tweak, my workflow for debugging feels transformed. It also solves the problem I had with figuring out when and how to test. I love the idea of taking this as an opportunity to increase testing coverage, whilst also assising in drilling down into which line of code.\nIt’s also great for those moments where I’m feeling stuck or uninspired for a moment. Answer the question, does this work as I think it should?\nIn this post I aim to outline an example from my work to illustrate the elements of a test:\nan expectation\na test that groups multiple expectations\na file that groups together multiple tests\nAt the end there is a little mathematical promenade I went on while working on this post. I find it useful to see other people’s learning process, and I didn’t want to just toss my notes away.\nA little example from what I’m working on\nSo, one of the things I need to code up properly at the moment is a series of densities with parameters estimated from summary statistic quantiles. I’m interested in comparing estimators derived from different distributions and how well they approximate the true density evaluated at the median.\nThis came up last night; I sat down to work and I had in my head that the exponential estimator needed debugging. So, I thought this was a perfect time to practice testing as debugging. Even if it does work as should, I will have increased testing coverage and not wasted my time. I will also feel both reassured and just a little bit smug that I levelled up my gitflow.\nConsider an exponential distribution \\(\\exp(\\lambda)\\) with rate parameter \\(\\lambda > 0\\). It can be shown1 that the median \\(\\mu\\) of this distribution is \\[\n\\mu := \\frac {\\log2}{\\lambda};\n\\] so, we can think of \\(\\lambda\\) in terms of \\(\\mu\\): \\[\n\\lambda = \\frac{\\log 2}{\\mu}.\n\\]\nI’d written a function exp_est the other day that I was pretty sure took a median \\(\\mu\\) as an argument, estimated \\(\\lambda\\) as above, and evaluated the exponential density \\(g\\) at \\(\\mu\\) with \\(\\lambda\\) estimated from the given median.\nSo, given an argument, some observed median \\(\\mu\\), we have the density evaluated at the median approximated thus \\[\ng(\\mu; \\lambda \\approx \\log 2/\\lambda)\n\\]\nThis calculation is wrapped up in the est_exp function. Seeing as I am just using this for the comparison, I’m not making this an exported funciton.\n\n\n\nGranted, this is only a couple of lines of code, so it can be eyeballed to make sure it’s commensurate with the mathematics2.\nExpectations\nFirstly, I wanted to ensure that the calculataion worked for the default parameters exp_est(qexp(1/2)). So, I expect that exp_est(qexp(1/2)) is equal to the density dexp evaluated at the median (calculated qexp(1/2))3.\nWe can do that by writing expectation. I believe the most common test is expect_equal.\n\n\n\nbut if I try this\n\n\n\nin the console I see this:\n\nError : 2 not equal to 4.\n1/1 mismatches\n[1] 2 - 4 == -2\nSo, I suppose this shows how the function expect_equal works, it checks to see if their difference is 0.\nI tried to think about a few ways it could go wrong. So, I tried specifying and not specifying the rate parameter, as well as checking a \\(0 < \\lambda < 1\\) value and a \\(\\lambda > 1\\) value.\nTests\nFor multiple tests that are alike in some way, in this case the same function, we can group them together into a test, a wrapper function testthat::test_that that takes in multiple expectations.\nWe group multiple tests in a file. For this type of stuff, where it wanders away from mathematics completely and focuses on the dev side of what we do, I tend to make good use of helper functions to automate as much as possible.\n\ntest_that(\"exponential estimator produces what it should\", {\n  expect_equal(exp(est(qexp(1/2)), dexp(qexp(1/2))))\n  expect_equal(exp_est(qexp(1/2)), dexp(qexp(1/2), log(2) / qexp(1/2)))\n  expect_equal(exp_est(qexp(1/2, rate = 3)), dexp(qexp(1/2, rate = 3), log(2) / qexp(1/2, rate = 3)))\n  expect_equal(exp_est(qexp(1/2, rate = 1/5)), dexp(qexp(1/2, rate = 1/5), log(2) / qexp(1/2, rate = 1/5)))\n})\nSyntax\nCuriously, we do not separate the different tests with ,, as an R coder of my ability expected. I separate mine with new lines, as all the examples do.\nI just experimented in the name of blogging posterity, and discovered that putting two tests on the same line with no space produced a parsing error that began thus:\n\nError in parse(textConnection(lines, encoding = \"UTF-8\"), n = -1, srcfile = srcfile,... \nSo it is not only easier to read, but is necessary to separate tests by new lines.\ntestthat makes testing fun!\nThere is a serious joy to running all tests (control + shift + t) and seeing that happy output:\n\n\n\nEvery time I hear Strongbad’s voice in my head saying “I make testing fun!”.\n\n\n\n\nTeaching myself maths as I go\nMy undergraduate degree was very light in analysis and calculus, which come up a lot in statistics. Most of what I focussed on was discrete mathematics. I find this helps immensely (even more than you’d think) with scientific programming, especially this new-fangled functional programming.\nI was a piano teacher for twenty years, and am by no means some undiscovered mathematical genius. I learn mathematics by painfully slowly unpicking definitions and playing around with them.\nThe following was for my own edification, probably has errors, certainly has fudgey bits, but was undeniably helpful getting my head back into a mathematical place.\nI believe it’s useful for people to see the learning process, that it’s not magic, it’s just study.\nMy blog is also intended to be a learning diary of sorts for myself, so I think it’ll be interesting to see how my understanding of concepts change and develop.\nFind the median of the exponential distribution\nWe want to show that the median \\(\\nu\\) of the exponential distribution \\(\\exp(\\lambda)\\) with rate parameter \\(\\lambda > 0\\) is given by \\[\n\\nu = \\frac{\\log2}{\\lambda}.\n\\]\nNow, we know that for any given pdf \\(f\\) we have a cdf \\(F\\) providing the probability that \\(X\\) takes up to a certain value \\(x\\), \\[\nF(x) = P(X \\leqslant x) = \\int_0^x f(x)\\, dx.\n\\]\nSo, where \\(F(x) = 1/2\\), we have \\(x = \\nu\\). Thus, solving \\(F(x) = 1/2\\) for \\(x\\) will provide us with the median \\(\\nu\\).\nNow, since \\(X \\sim \\exp(\\lambda)\\), we have \\[\nf(x) := \n\\begin{cases}\n\\lambda e^{- \\lambda x} & x \\geqslant 0\\\\\n0 & x < 0\n\\end{cases}\n\\] with \\(\\lambda > 0\\).\nThus, we need to solve for \\(x\\), \\[\n \\int_0^x \\lambda e^{- \\lambda x} \\, dx = \\frac 1 2.\n\\]\nSo, if I can find \\(\\int e^{ax}\\, dx\\) for some \\(a > 0\\), then I am most of the way to solving this problem.\nSomething I love about the exponential function is its series definition, \\[\ne^x = \\exp(x) = \\sum_{k = 0}^\\infty \\frac{x^k}{k!} = 1 + x + \\frac{x^2}{2} + \\frac{x^3}{6} + \\dots\n\\] (In particular, I like this definition because it helps me understand Euler’s formula \\(e^{ix} = \\cos x + i \\sin x\\), using the series definitions of sine and cosine.)\nSince this is a polynomial, we can differentiate termwise, \\[\n\\frac{de^x}{dx} = 0 + 1 + \\frac{2x}{2} + \\frac{3x^2}{6} \\dots = 1 + x + \\frac{x^2}{2} + \\dots\n\\] thus giving the lovely result, \\(\\frac{de^x}{dx} = e^x\\). I love how intuitive this is in series form. I’d completely forgotten it. Once gets so used to \\(\\frac{de^x}{dx} = e^x\\), that one forgets why. So many things in maths are like that for me. And then I find it so irritating that I have to go through it all again.\nNow, starting with the chain rule we have\n\\[\\begin{align*}\n\\frac{de^{ax}}{dx} & = \\frac{de^{ax}}{d(ax)} \\cdot \\frac{dax}{dx} = ae^{ax} \\\\\n\\implies \\int \\frac{de^{ax}}{dx} & = a \\int  e^{ax}\\, dx.\n\\end{align*}\\]\nTo know how to handle integrating a derivative, we turn to the fundamental theorem of calculus, which states that if \\(f\\) is a real-valued function on a closed interval \\([a,b]\\), then \\(F(x) = \\int_a^b f(x)\\, dx\\) which gives \\(F'(x) = f(x)\\). Or something, hm, perhaps that’s not quite right. It’s some fundamental application of calculus that gives us \\(\\int \\frac{d}{dx}f(x) = f(x)\\).\nAnyway, we now have\n\\[\\begin{align*}\n\\int \\frac{de^{ax}}{dx}  & = a \\int  e^{ax}\\, dx \\\\\n\\implies e^{ax} & = a \\int e^{ax} dx. \\\\\n\\int e^{ax} dx & = a^{-1}e^{ax}.\\ (*)\n\\end{align*}\\]\nI think I’m glossing past some assumptions about this only working for definite integrals, but when we actually apply it we will on definite integrals, so I can live with this fudgery as we now have the key to the problem.\nReturning to our original problem, then, we have\n\\[\\begin{align*}\n\\int_0^{x} \\lambda e^{-\\lambda x}\\, dx & = \\frac{1}{2} \\\\\n\\implies \\lambda \\int_0^{x} e^{-\\lambda x}\\, dx & = \\frac{1}{2} \\\\\n\\implies \\lambda((-\\lambda)^{-1} e^{-\\lambda x} - (-\\lambda)^{-1}) & = \\frac 1 2\\\\\n\\implies - e^{-\\lambda x} + 1 & = \\frac{1}{2} \\\\\n\\implies e^{- \\lambda x} & = \\frac 1 2 \\\\\n\\implies - \\lambda x & = \\log(1/2)\\\\\n\\implies x & = \\log 2 / \\lambda.\n\\end{align*}\\]\nAnd since this came from the assumption \\(F(x) = 1/2\\), we now have that the median \\(\\nu\\) of the exponential distribution is given by \\(\\nu = \\log2/\\lambda\\), as required. :)\nThis actually bugged me all week. So, I’ve added my mathematical musings as the final section. I↩\nI love converting my code back to maths as a lowfi debugging practice.↩\nI like to make calculations in fractions to remind myself that I’m dealing with standard quantiles, such as the median or quartiles.↩\n",
    "preview": {},
    "last_modified": "2021-01-19T01:18:21+00:00",
    "input_file": {}
  }
]
