---
title: "chapter 2"
description: |
  exercises from chapter 2 statistical rethinking
date: "`r Sys.Date()`"
output: 
    distill::distill_article:
      toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Checking solutions with this [helpful discussion](https://sr2-solutions.wjakethompson.com/bayesian-inference.html#exercises) and [this other set of solutions](https://jmgirard.com/statistical-rethinking-ch2/). Yay for open learning. 

Last updated: `r Sys.Date()`.

## easy

### 2E1

> Which of the expressions below correspond to the statement: the probability of rain on Monday?

1. P(rain)
2. P(rain|Monday)
3. P(Monday|rain)
4. P(rain, Monday) / P(Monday)

Adapting the equation on p. 37 to a more generalised statement, for a parameter of interest $\theta$ and observations $y$ dependent on that parameter, we have

$$
P(\theta | y) = \frac{P(y|\theta)P(\theta)}{P(y)} = P(\theta, y)/P(y)
$$

Then P(rain|Monday) = P(rain, Monday) / P(Monday).

So, I conclude that 2. and 4. correspond to the statement.

### 2E2

The following statement corresponds to the expression P(Monday|rain):

3. The probability that it is Monday, given that it is raining. 

### 2E3

> The probability that it is Monday given that it is raining.

Can be expressed


P(Monday|rain) = P(rain|Monday)P(Monday)/P(rain)

So, the following statements correspond.

1. P(Monday|rain)
and 4. P(rain|Monday)P(Monday)/P(rain)

### 2E4

> The Bayesian statistician Finetti began his book on probablity theory with the declaration: "PROBABILITY DOES NOT EXIST". What he meant is that probability is a device for describing uncertainty from the perspective ofg an observer with limited knowledge; it has no objective reality. What does it mean to say, "the probability of water is 0.7"?

Based on the observations we have of water and land from globe tossing, the most credible proportion of the globe is approximately 0.7. 

## medium

### 2M1

> Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for **p**.

1. WWW
2. WWWL
3. LWWLWWW

We assume 
$$
\begin{array}
\\W & \sim & \text{binomial}(N, p)\\
p & \sim & \text{uniform}(0, 1)
\end{array}
$$
where $W$ is the number of water observations and $N = L + W$ is the total number of water and land observations. We only know $p$ is a proportion, so we know that it falls in $[0,1]$.

We are interested in finding
$$
P(p|W,L) = P(W,L|p)P(p)/P(W,L)
$$
where $P(p|W,L)$ is the posterior, $P(W,L|p)$ is the probability of the data, $P(W,L)$ is the prior.

> And this is Bayes' theorem. It says the the probability of any particular value of $p$, considering the data is equal to the product of the relative plausibility of the data, conditional on $p$, and the prior plausibility of $p$, divided by the average probability of the data. 

$$
\text{Posterior} = \frac{
\text{Probability of the data} \times \text{Prior}
}{
\text{Average probability of the data}
}
$$
```{r WWW, eval=TRUE}

library(tidyverse)

# set number of points for the grid 
n <- 10

# create a grid approximation tableT
www <- 
    tibble(
        # define grid
        p = seq(0, 1, length.out = n),
        # compute prior at each parameter value
        prior = 1
    ) %>% 
    mutate(
        # compute the likelihood at each parameter value
        plausibility = dbinom(3, size = 3, prob = p),
        # comput the unstandardised posterior
        unstd_post = plausibility * prior,
        # standardise the posterior
        posterior = unstd_post / sum(unstd_post)
    )

```

```{r}

grid_plot <- function(post_df, title_text) {
post_df %>% 
    ggplot(
        aes(
            x = p,
            y = posterior
        )
    ) +
    geom_line(alpha = 0.4) +
    geom_point(alpha = 0.8) + 
    labs(title = title_text) + 
        ggthemes::theme_tufte()
    
}



```

It makes sense that for `www` observation, we have a monotonically increasing posterior. Higher values of $p$ are more likely, given we have _only_ observed water.


```{r}
grid_plot(www, "w w w")
```

Now for the second set of observations, `wwwl`. This should be slightly more interesting, as there is one land observation, so we know with certainty that $p \neq 1$, whereas in `www`, this was the most plausible value for $p$.

```{r}

wwwl <- 
    tibble(
        # define grid
        p = seq(0, 1, length.out = n),
        # compute prior at each parameter value
        prior = 1
    ) %>% 
    mutate(
        # compute the likelihood at each parameter value
        plausibility = dbinom(3, size = 4, prob = p),
        # comput the unstandardised posterior
        unstd_post = plausibility * prior,
        # standardise the posterior
        posterior = unstd_post / sum(unstd_post)
    )

grid_plot(wwwl, "w w w l")

```

```{r}
lwwlwww <- 
    tibble(
        # define grid
        p = seq(0, 1, length.out = n),
        # compute prior at each parameter value
        prior = 1
    ) %>% 
    mutate(
        # compute the likelihood at each parameter value
        plausibility = dbinom(5, size = 7, prob = p),
        # comput the unstandardised posterior
        unstd_post = plausibility * prior,
        # standardise the posterior
        posterior = unstd_post / sum(unstd_post)
    )

grid_plot(lwwlwww, "l w w l w w")


```

### 2M2

We now assume a prior $P: [0,1] \to \{0, \theta\}$ such that 
$$
p \mapsto 
\begin{cases}
0 & p < 0.5\\
\theta & p \geqslant 0.5
\end{cases}
$$
where $\theta \in \mathbb R^+$, and again compute the grid approximations as in 2M1.

Now I've got the general idea of each line of the grid approximation, I'll now functionalise it. 

```{r grid approx}
grid_theta <- function(w, sample_size) {
  # get a random positive value
  theta <- runif(1, 1, 10000)
  
  # return tibble of grid approx
     tibble(
        # define grid
        p = seq(0, 1, length.out = n),
        # compute prior at each parameter value
        prior = if_else(p < 0.5, 0, theta)
    ) %>% 
    mutate(
        # compute the likelihood at each parameter value
        plausibility = dbinom(w, size = sample_size, prob = p),
        # comput the unstandardised posterior
        unstd_post = plausibility * prior,
        # standardise the posterior
        posterior = unstd_post / sum(unstd_post)
    )
}
  
```

```{r}
# www

grid_theta(3, 3) %>% 
  grid_plot("w w w, prior = 0 for p < 0.5")

```
```{r}
# wwwl

grid_theta(3, 4) %>% 
  grid_plot("w w w l, prior = 0 for p < 0.5")

```

```{r}
# lwwlwww
grid_theta(5, 7) %>% 
  grid_plot("l w w l w w w, prior = 0 for p < 0.5")


```

Since 
$$
\text{Posterior} = \frac{
\text{Probability of the data} \times \text{Prior}
}{
\text{Average probability of the data}
}
$$
In all three questions, for $p< 0.5$, the posterior is 0.

Since the numerator is a product of the probability of the data and the prior, which is set to 0 for $p < 0.5$, and any product containing zero is zero, we have a posterior of 0 for all values $p < 0.5$. That is, where the prior is 0, this will *always* produce a posterior of 0, regardless of the value of the probability of the data.  

### 2M3

> Suppose there are two globes, one for Earth and one for Mars. The Earth globe is 70% covered in water. The Mars globe is 100% land.

So, P(water|Earth) = 0.7 and P(land|Mars) = 1.0.

> Further suppose that one of these globes - you don't know which - was tossed in the air and produced a land observation. Assume each globe was equally likely to be tossed. 

So, P(Earth) = 0.5 and P(Mars) = 0.5.

> Show that the posterior probability that the globe was Earth conditional on seeing land is 0.23, that is, P(Earth|land) = 0.23. 

We wish to show that P(Earth|land) = 0.23. 

Well, Bayes' theorem gives us,

P(Earth|land) = P(land|Earth)P(Earth)/P(land). 

So we need to find P(land|Earth), P(Earth), and P(land).

We know P(Earth) = 0.5.

Now P(water|Earth) = 0.7, so P(land|Earth) = 0.3, since there are only two possibilities on Earth. 

Finally, for P(land), we note that, going back to the original generalised notation,
$$
P(y) = E(P(y|\theta)) = \int P(y|\theta)P(\theta)d\theta
$$
so, since in this example we are dealing with discrete possibilities,

P(land) = E(P(land|Planet)) = $\displaystyle\sum_{\text{Planet } \in \{\text{Earth},\text{Mars}\}}$ P(land|Planet)P(planet) = P(land|Earth) P(Earth) + P(land|Mars) P(Mars).

Then P(land) is

```{r}
0.3 * 0.5 + 1 * 0.5

```

So, we have P(land|Earth), by Bayes' theorem, is

```{r}
(0.3 * 0.5)/(0.3 * 0.5 + 1 * 0.5)
```

### 2M4 

> Suppose you have a deck with only three cards. 

Let $X$ denote the set of three cards, so $|X|=3$.

> And each side is either black or white. One card has two black sides. The second card has one black and one white side. The third card has two white sides. 

So, we could say $X = \{BB, BW, WW\}$. 

> Someone pulls out a card and places it flat on a table. A black side is shown facing up, but you don't know the colour of the side facing down. 

Let $y$ denote the observation $B$. Now, clearly $P(B|WW) = 0$. 

> Use the counting method from Section 2. This means counting up the ways that each card could produce the observed data. 

> Show that the probability that the other side is also black is 2/3. 

### 2M5

Now suppose there are four cards $\{BB, BW, WW, BB\}$. 

Again calculate the probability the other side is black. 




## hard